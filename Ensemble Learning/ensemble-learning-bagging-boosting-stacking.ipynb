{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022517,
     "end_time": "2021-12-27T07:06:22.372344",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.349827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Ensemble_Learning](https://raw.githubusercontent.com/satishgunjal/images/master/Ensemble_Learning.png)\n",
    "\n",
    "# Index\n",
    "* [Introduction](#1)\n",
    "* [Bagging](#2)\n",
    "* [Boosting](#3)\n",
    "* [Stacking](#4)\n",
    "* [Python Example](#5)\n",
    "  - [Import Libraries](#6)\n",
    "  - [Load Data](#7)\n",
    "  - [Train and Test Data](#8)\n",
    "  - [Modeling](#9)\n",
    "    - [Linear Regression](#10)\n",
    "\t- [Lasso Regression](#11)\n",
    "\t- [ElasticNet Regression](#12)\n",
    "\t- [KernelRidge Regression](#13)\n",
    "  - [Ensemble Modeling](#14)\n",
    "    - [Bagging](#15)\n",
    "\t- [Boosting](#16)\n",
    "\t  - [GradientBoostingRegressor](#17)\n",
    "\t  - [XGBRegressor](#18)\n",
    "\t  - [LGBMRegressor](#19)\n",
    "\t- [Stacking](#20)\n",
    "    \n",
    "# Introduction <a id= \"1\"></a>\n",
    "\n",
    "Whenever we make any important decision we first discuss it with friends, family or an expert. Nowadays we check the reviews on social media or check a YouTube video. Considering other people's opinion just make final decision more informed and make sure to avoid any kind of surprises as we are combining multiple opinions about the same thing together. \n",
    "\n",
    "Ensemble modeling in machine learning operates on the same principle, where we combine the predictions from multiple models to generate the final model which provide better overall performance. Ensemble modeling helps to generalize the learning based on training data, so that it will be able to do predictions accurately on unknown data. \n",
    "\n",
    "Modeling is one of the most important step in machine learning pipeline. The main motivation behind ensemble learning is to correctly combine weak models to get a more accurate and robust model with bias-variance trade off. For example Random Forest algorithm is ensemble of Decision Tree and since it combine multiple decision  tree models it always perform better than single decision tree model.\n",
    "\n",
    "Depending on how we combine the base models, ensemble learning can be classified in three different types Bagging, Boosting and  Stacking.\n",
    "\n",
    "* **Bagging**: The working principle is to build several base models independently and then to average them for final predictions. \n",
    "* **Boosting**: Boosting models are built sequentially and tries to reduce the bias on final predictions. \n",
    "* **Stacking**: The predictions of each individual model are stacked together and used as input to a final estimator to compute the prediction. \n",
    " \n",
    "Ensemble learning approach makes the model more robust and helps to achieve the better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020293,
     "end_time": "2021-12-27T07:06:22.413517",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.393224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bagging <a id= \"2\"></a>\n",
    "\n",
    "![EnsembleI_Learning_Bagging](https://raw.githubusercontent.com/satishgunjal/images/master/Ensemble_Learning_Bagging.png)\n",
    "\n",
    "* In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions.\n",
    "* Bagging is a short form of **B*ootstrap *Agg*regat*ing*. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms.\n",
    "* Since multiple model predictions are averaged together to form the final predictions, Bagging reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. \n",
    "* Bagging is a special case of the model averaging approach, in case of regression problem we take mean of the output and in case of classification we take the majority vote. \n",
    "* Bagging is more helpfull if we have over fitting (high variance) base models.\n",
    "* We can also build independent estimators of same type on each subset. These independent estimators also enable us to parallelly process and increase the speed.\n",
    "* Most popular bagging estimator is 'Bagging Tress' also knows as 'Random Forest'\n",
    "\n",
    "**Bootstrapping**\n",
    "* It is a resampling technique, where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample.\n",
    "* So this technique will enable us to produce as many subsample as we required from the original training data.\n",
    "* So the defination is simple to understand, but \"replacement\" word may be confusing sometimes. Here 'replacement' word signifies that the same obervation may repeat more than once in a given sample, and hence this technique is also known as **sampleing with replacement**\n",
    "\n",
    "![Bootstrap_Sampling_ML](https://raw.githubusercontent.com/satishgunjal/images/master/Bootstrap_Sampling_ML.png)\n",
    "\n",
    "* As you can see in above image we have training data with observations from X1 to X10. In first bootstrap training sample X6, X10 and X2 are repeated where as in second training sample X3, X4, X7 and X9 are repeated.\n",
    "* Bootstrap sampling helps us to generate random sample from given training data for each model in order to genralise the final estimation.\n",
    "\n",
    "So in case of Bagging we create multiple number of bootstrap samples from given data to train our base models. Each sample will contain training and test data sets which are different from each other and remember that training sample may contain duplicate observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020215,
     "end_time": "2021-12-27T07:06:22.454305",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.434090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Boosting <a id= \"3\"></a>\n",
    "* In case of boosting, machine learning models are used one after the other and the predictions made by first layer models are used as input to next layer models. The last layer of models will use the predictions from all previous layers to get the final predictions. \n",
    "* So boosting enables each subsequent model to boost the performance of the previous one by overcomming or reducing the error of the previous model.\n",
    "* Unlike bagging, in case of boosting the base learners are trained in sequence on a weighted version of the data. Boosting is more helpful if we have biased base models.\n",
    "* Boosting can be used to solve regression and classification problems.\n",
    "\n",
    "![Ensemble_Learning_Boosting](https://raw.githubusercontent.com/satishgunjal/images/master/Ensemble_Learning_Boosting.png)\n",
    "\n",
    "Different types of Boosting algorithms\n",
    "* Gradient Boosting Machine (GBM)\n",
    "* Extreme Gradient Boosting Machine (XGBM)\n",
    "* LightGBM\n",
    "* CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020313,
     "end_time": "2021-12-27T07:06:22.495273",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.474960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stacking <a id= \"4\"></a>\n",
    "Model stacking is a method for combining models to reduce their biases. The predictions of each individual model are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation.\n",
    "\n",
    "Note that in case of stacking we use heterogeneous weak learners (different learning algorithms) but in case bagging and boosting we mainly use homogeneous weak learners. \n",
    "\n",
    "![Ensemble_Learning_Stacking](https://raw.githubusercontent.com/satishgunjal/images/master/Ensemble_Learning_Stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02035,
     "end_time": "2021-12-27T07:06:22.536159",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.515809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# When to use Ensemble Learning?\n",
    "Since Ensemble learning results in better accuracy, high consistency and also helps to avoid bias variance tradeoff should'nt we use it everywhere? The short answer is it depends on the problem in hand. If our model with available training data is not performing well and showing the signs of overfitting/unterfitting and additinal compute power is not an issue then going for Ensemble Learning is best option. However one shouldnt skip the first steps of improving the input data and trying different hyperparmeters before going for ensemple approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020283,
     "end_time": "2021-12-27T07:06:22.577098",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.556815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Python Example <a id= \"5\"></a>\n",
    "\n",
    "We are going to use [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data. Our objective is to predict the final price of each house based on the 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. We will try all the ensemble learning approaches and compare their results.\n",
    "\n",
    "![House_Prices_Advanced_Regression_Techniques](https://raw.githubusercontent.com/satishgunjal/images/master/House_Prices_Advanced_Regression_Techniques.png)\n",
    "\n",
    "## Import Libraries <a id= \"6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:22.622942Z",
     "iopub.status.busy": "2021-12-27T07:06:22.622221Z",
     "iopub.status.idle": "2021-12-27T07:06:24.595707Z",
     "shell.execute_reply": "2021-12-27T07:06:24.594366Z"
    },
    "papermill": {
     "duration": 1.998101,
     "end_time": "2021-12-27T07:06:24.595845",
     "exception": false,
     "start_time": "2021-12-27T07:06:22.597744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Global settings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # To ignore warnings\n",
    "n_jobs = -1 # This parameter conrols the parallel processing. -1 means using all processors.\n",
    "random_state = 42 # This parameter controls the randomness of the data. Using some int value to get same results everytime this code is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02086,
     "end_time": "2021-12-27T07:06:24.638040",
     "exception": false,
     "start_time": "2021-12-27T07:06:24.617180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data <a id= \"7\"></a>\n",
    "Since the objective of this article is to test the different ensemble techniques, I have excluded the data preprocessing and EDA steps. I am going to use model ready dataset, so that we can straight away start modeling and ensembling. **X.csv** contains all the training data and **y.csv** contains the label values. In this case \"SalePrice\" is the label/target variable which represent the property's sale price in dollars that we are trying to predict.\n",
    "\n",
    "Below is the list of features:\n",
    "* MSSubClass: The building class\n",
    "* MSZoning: The general zoning classification\n",
    "* LotFrontage: Linear feet of street connected to property\n",
    "* LotArea: Lot size in square feet\n",
    "* Street: Type of road access\n",
    "* Alley: Type of alley access\n",
    "* LotShape: General shape of property\n",
    "* LandContour: Flatness of the property\n",
    "* Utilities: Type of utilities available\n",
    "* LotConfig: Lot configuration\n",
    "* LandSlope: Slope of property\n",
    "* Neighborhood: Physical locations within Ames city limits\n",
    "* Condition1: Proximity to main road or railroad\n",
    "* Condition2: Proximity to main road or railroad (if a second is present)\n",
    "* BldgType: Type of dwelling\n",
    "* HouseStyle: Style of dwelling\n",
    "* OverallQual: Overall material and finish quality\n",
    "* OverallCond: Overall condition rating\n",
    "* YearBuilt: Original construction date\n",
    "* YearRemodAdd: Remodel date\n",
    "* RoofStyle: Type of roof\n",
    "* RoofMatl: Roof material\n",
    "* Exterior1st: Exterior covering on house\n",
    "* Exterior2nd: Exterior covering on house (if more than one material)\n",
    "* MasVnrType: Masonry veneer type\n",
    "* MasVnrArea: Masonry veneer area in square feet\n",
    "* ExterQual: Exterior material quality\n",
    "* ExterCond: Present condition of the material on the exterior\n",
    "* Foundation: Type of foundation\n",
    "* BsmtQual: Height of the basement\n",
    "* BsmtCond: General condition of the basement\n",
    "* BsmtExposure: Walkout or garden level basement walls\n",
    "* BsmtFinType1: Quality of basement finished area\n",
    "* BsmtFinSF1: Type 1 finished square feet\n",
    "* BsmtFinType2: Quality of second finished area (if present)\n",
    "* BsmtFinSF2: Type 2 finished square feet\n",
    "* BsmtUnfSF: Unfinished square feet of basement area\n",
    "* TotalBsmtSF: Total square feet of basement area\n",
    "* Heating: Type of heating\n",
    "* HeatingQC: Heating quality and condition\n",
    "* CentralAir: Central air conditioning\n",
    "* Electrical: Electrical system\n",
    "* 1stFlrSF: First Floor square feet\n",
    "* 2ndFlrSF: Second floor square feet\n",
    "* LowQualFinSF: Low quality finished square feet (all floors)\n",
    "* GrLivArea: Above grade (ground) living area square feet\n",
    "* BsmtFullBath: Basement full bathrooms\n",
    "* BsmtHalfBath: Basement half bathrooms\n",
    "* FullBath: Full bathrooms above grade\n",
    "* HalfBath: Half baths above grade\n",
    "* Bedroom: Number of bedrooms above basement level\n",
    "* Kitchen: Number of kitchens\n",
    "* KitchenQual: Kitchen quality\n",
    "* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "* Functional: Home functionality rating\n",
    "* Fireplaces: Number of fireplaces\n",
    "* FireplaceQu: Fireplace quality\n",
    "* GarageType: Garage location\n",
    "* GarageYrBlt: Year garage was built\n",
    "* GarageFinish: Interior finish of the garage\n",
    "* GarageCars: Size of garage in car capacity\n",
    "* GarageArea: Size of garage in square feet\n",
    "* GarageQual: Garage quality\n",
    "* GarageCond: Garage condition\n",
    "* PavedDrive: Paved driveway\n",
    "* WoodDeckSF: Wood deck area in square feet\n",
    "* OpenPorchSF: Open porch area in square feet\n",
    "* EnclosedPorch: Enclosed porch area in square feet\n",
    "* 3SsnPorch: Three season porch area in square feet\n",
    "* ScreenPorch: Screen porch area in square feet\n",
    "* PoolArea: Pool area in square feet\n",
    "* PoolQC: Pool quality\n",
    "* Fence: Fence quality\n",
    "* MiscFeature: Miscellaneous feature not covered in other categories\n",
    "* MiscVal: $Value of miscellaneous feature\n",
    "* MoSold: Month Sold\n",
    "* YrSold: Year Sold\n",
    "* SaleType: Type of sale\n",
    "* SaleCondition: Condition of sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:24.683051Z",
     "iopub.status.busy": "2021-12-27T07:06:24.682384Z",
     "iopub.status.idle": "2021-12-27T07:06:24.786365Z",
     "shell.execute_reply": "2021-12-27T07:06:24.786962Z"
    },
    "papermill": {
     "duration": 0.128272,
     "end_time": "2021-12-27T07:06:24.787132",
     "exception": false,
     "start_time": "2021-12-27T07:06:24.658860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X= (1458, 220)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.885846</td>\n",
       "      <td>5.831328</td>\n",
       "      <td>19.212182</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.440268</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>14.187527</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.055642</td>\n",
       "      <td>6.221214</td>\n",
       "      <td>19.712205</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.259674</td>\n",
       "      <td>2.440268</td>\n",
       "      <td>14.145138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.885846</td>\n",
       "      <td>5.914940</td>\n",
       "      <td>20.347241</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.440268</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>14.184404</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.011340</td>\n",
       "      <td>5.684507</td>\n",
       "      <td>19.691553</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.440268</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>14.047529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.885846</td>\n",
       "      <td>6.314735</td>\n",
       "      <td>21.325160</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.602594</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>14.182841</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage    LotArea    Street     Alley  LotShape  \\\n",
       "0    2.885846     5.831328  19.212182  0.730463  0.730463  1.540963   \n",
       "1    2.055642     6.221214  19.712205  0.730463  0.730463  1.540963   \n",
       "2    2.885846     5.914940  20.347241  0.730463  0.730463  0.000000   \n",
       "3    3.011340     5.684507  19.691553  0.730463  0.730463  0.000000   \n",
       "4    2.885846     6.314735  21.325160  0.730463  0.730463  0.000000   \n",
       "\n",
       "   LandSlope  OverallQual  OverallCond  YearBuilt  ...  SaleType_ConLw  \\\n",
       "0        0.0     2.440268     1.820334  14.187527  ...               0   \n",
       "1        0.0     2.259674     2.440268  14.145138  ...               0   \n",
       "2        0.0     2.440268     1.820334  14.184404  ...               0   \n",
       "3        0.0     2.440268     1.820334  14.047529  ...               0   \n",
       "4        0.0     2.602594     1.820334  14.182841  ...               0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0             0             0            1                      0   \n",
       "1             0             0            1                      0   \n",
       "2             0             0            1                      0   \n",
       "3             0             0            1                      1   \n",
       "4             0             0            1                      0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                      0                     0                     0   \n",
       "1                      0                     0                     0   \n",
       "2                      0                     0                     0   \n",
       "3                      0                     0                     0   \n",
       "4                      0                     0                     0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     1                      0  \n",
       "1                     1                      0  \n",
       "2                     1                      0  \n",
       "3                     0                      0  \n",
       "4                     1                      0  \n",
       "\n",
       "[5 rows x 220 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('/kaggle/input/modelling-ready-data/X.csv')\n",
    "print(f'Shape of X= {X.shape}')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:24.838298Z",
     "iopub.status.busy": "2021-12-27T07:06:24.837442Z",
     "iopub.status.idle": "2021-12-27T07:06:24.850761Z",
     "shell.execute_reply": "2021-12-27T07:06:24.850083Z"
    },
    "papermill": {
     "duration": 0.040881,
     "end_time": "2021-12-27T07:06:24.850879",
     "exception": false,
     "start_time": "2021-12-27T07:06:24.809998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y= (1458, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  12.247699\n",
       "1  12.109016\n",
       "2  12.317171\n",
       "3  11.849405\n",
       "4  12.429220"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv('/kaggle/input/modelling-ready-data/y.csv')\n",
    "print(f'Shape of y= {y.shape}')\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023685,
     "end_time": "2021-12-27T07:06:24.897777",
     "exception": false,
     "start_time": "2021-12-27T07:06:24.874092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Train and Test Data <a id= \"8\"></a>\n",
    "We will use train_test_split() method to create training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:24.955105Z",
     "iopub.status.busy": "2021-12-27T07:06:24.952095Z",
     "iopub.status.idle": "2021-12-27T07:06:24.961417Z",
     "shell.execute_reply": "2021-12-27T07:06:24.961899Z"
    },
    "papermill": {
     "duration": 0.041392,
     "end_time": "2021-12-27T07:06:24.962106",
     "exception": false,
     "start_time": "2021-12-27T07:06:24.920714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set--> X_train shape= (976, 220), y_train shape= (976, 1)\n",
      "Holdout set--> X_test shape= (482, 220), y_test shape= (482, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state = random_state)\n",
    "\n",
    "print(f'Training set--> X_train shape= {X_train.shape}, y_train shape= {y_train.shape}')\n",
    "print(f'Holdout set--> X_test shape= {X_test.shape}, y_test shape= {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023235,
     "end_time": "2021-12-27T07:06:25.008975",
     "exception": false,
     "start_time": "2021-12-27T07:06:24.985740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling <a id= \"9\"></a>\n",
    "* I am also skipping the hyperparameter tuning and used already tuned hyperparameters here\n",
    "* We will use multiple models individually as well as in ensemble mode to test the final predictions.\n",
    "* We are going to use **Root Mean Square Error(RMSE)** metric to compare the scores. Since this metrics is not available out of the box we will create a function for it.\n",
    "\n",
    "Note: RMSE metric is used to express the loss in the same unit of measurement as label value, in this case house price in dollars. For example if RMSE for house price is 2, then we can loosely interpret it as 'on average incorrect predictions are wrong by around 2 house prices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:25.063474Z",
     "iopub.status.busy": "2021-12-27T07:06:25.062371Z",
     "iopub.status.idle": "2021-12-27T07:06:25.065953Z",
     "shell.execute_reply": "2021-12-27T07:06:25.065361Z"
    },
    "papermill": {
     "duration": 0.033611,
     "end_time": "2021-12-27T07:06:25.066081",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.032470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_scores = [] # To store model scores\n",
    "\n",
    "def rmse(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return mean_squared_error(y_test, y_pred, squared= False) # squared= False > returns Root Mean Square Error                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023242,
     "end_time": "2021-12-27T07:06:25.113943",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.090701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Linear Regression <a id= \"10\"></a>\n",
    "\n",
    "* Lest test using Ordinary least squares Linear Regression.\n",
    "* LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:25.172200Z",
     "iopub.status.busy": "2021-12-27T07:06:25.171016Z",
     "iopub.status.idle": "2021-12-27T07:06:25.280957Z",
     "shell.execute_reply": "2021-12-27T07:06:25.281650Z"
    },
    "papermill": {
     "duration": 0.144133,
     "end_time": "2021-12-27T07:06:25.281864",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.137731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression Score= 0.13179065013062716\n"
     ]
    }
   ],
   "source": [
    "linear_regression = make_pipeline(LinearRegression())\n",
    "score = rmse(linear_regression)\n",
    "\n",
    "models_scores.append(['LinearRegression', score])\n",
    "print(f'LinearRegression Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024151,
     "end_time": "2021-12-27T07:06:25.332752",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.308601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lasso Regression <a id= \"11\"></a>\n",
    "* The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. \n",
    "* This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:25.385677Z",
     "iopub.status.busy": "2021-12-27T07:06:25.384715Z",
     "iopub.status.idle": "2021-12-27T07:06:25.498539Z",
     "shell.execute_reply": "2021-12-27T07:06:25.499163Z"
    },
    "papermill": {
     "duration": 0.142117,
     "end_time": "2021-12-27T07:06:25.499344",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.357227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Score= 0.1110532696244382\n"
     ]
    }
   ],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state= random_state))\n",
    "\n",
    "score = rmse(lasso)\n",
    "models_scores.append(['Lasso', score])\n",
    "print(f'Lasso Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024278,
     "end_time": "2021-12-27T07:06:25.552550",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.528272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ElasticNet Regression <a id= \"12\"></a>\n",
    "* Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "* A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge’s stability under rotation.\n",
    "* This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:25.605628Z",
     "iopub.status.busy": "2021-12-27T07:06:25.604603Z",
     "iopub.status.idle": "2021-12-27T07:06:25.710877Z",
     "shell.execute_reply": "2021-12-27T07:06:25.710342Z"
    },
    "papermill": {
     "duration": 0.133944,
     "end_time": "2021-12-27T07:06:25.711044",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.577100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Score= 0.11107756118615623\n"
     ]
    }
   ],
   "source": [
    "elastic_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio= .9, random_state= random_state))\n",
    "\n",
    "score = rmse(elastic_net)\n",
    "models_scores.append(['ElasticNet', score])\n",
    "print(f'ElasticNet Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0251,
     "end_time": "2021-12-27T07:06:25.761924",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.736824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### KernelRidge Regression <a id= \"13\"></a>\n",
    "* Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:25.817357Z",
     "iopub.status.busy": "2021-12-27T07:06:25.816366Z",
     "iopub.status.idle": "2021-12-27T07:06:25.897733Z",
     "shell.execute_reply": "2021-12-27T07:06:25.898441Z"
    },
    "papermill": {
     "duration": 0.110345,
     "end_time": "2021-12-27T07:06:25.898621",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.788276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelRidge Score= 0.13639161839448324\n"
     ]
    }
   ],
   "source": [
    "kernel_ridge= KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "score = rmse(kernel_ridge)\n",
    "models_scores.append(['KernelRidge', score])\n",
    "print(f'KernelRidge Score= {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:25.961355Z",
     "iopub.status.busy": "2021-12-27T07:06:25.960337Z",
     "iopub.status.idle": "2021-12-27T07:06:25.973168Z",
     "shell.execute_reply": "2021-12-27T07:06:25.973711Z"
    },
    "papermill": {
     "duration": 0.042862,
     "end_time": "2021-12-27T07:06:25.973865",
     "exception": false,
     "start_time": "2021-12-27T07:06:25.931003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.111053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.111078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.131791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KernelRidge</td>\n",
       "      <td>0.136392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1\n",
       "1             Lasso  0.111053\n",
       "2        ElasticNet  0.111078\n",
       "0  LinearRegression  0.131791\n",
       "3       KernelRidge  0.136392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking the scores of each model\n",
    "pd.DataFrame(models_scores).sort_values(by=[1], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026123,
     "end_time": "2021-12-27T07:06:26.027004",
     "exception": false,
     "start_time": "2021-12-27T07:06:26.000881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Modeling <a id= \"14\"></a>\n",
    "\n",
    "### Bagging <a id= \"15\"></a>\n",
    "* We are going to use sklearns \"**BaggingRegressor**\" to fit the base regressors (LinearRegression, Lasso, ElasticNet, KernelRidge)\n",
    "* A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n",
    "* In particular, **max_samples** and **max_features** control the size of the subsets (in terms of samples and features), while **bootstrap** and **bootstrap_features** control whether samples and features are drawn with or without replacement.\n",
    "* We are using 10 base estimators in ensemble. \n",
    "* Method bagging_predictions() calculate the score of each base estimator against the test data and also returns the test prediction values.\n",
    "* Using **column_stack** we will store the predictions for each base estimator in a separate column and then take the average of all the predictions for final RMSE calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:26.084444Z",
     "iopub.status.busy": "2021-12-27T07:06:26.083759Z",
     "iopub.status.idle": "2021-12-27T07:06:29.046301Z",
     "shell.execute_reply": "2021-12-27T07:06:29.047237Z"
    },
    "papermill": {
     "duration": 2.99362,
     "end_time": "2021-12-27T07:06:29.047424",
     "exception": false,
     "start_time": "2021-12-27T07:06:26.053804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for base estimator Pipeline(steps=[('linearregression', LinearRegression())]) = 0.12130723991069779\n",
      "\n",
      "RMSE for base estimator Pipeline(steps=[('robustscaler', RobustScaler()),\n",
      "                ('lasso', Lasso(alpha=0.0005, random_state=42))]) = 0.10947971499614259\n",
      "\n",
      "RMSE for base estimator Pipeline(steps=[('robustscaler', RobustScaler()),\n",
      "                ('elasticnet',\n",
      "                 ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42))]) = 0.10955384344423397\n",
      "\n",
      "RMSE for base estimator KernelRidge(alpha=0.6, coef0=2.5, degree=2, kernel='polynomial') = 0.14435769550845381\n",
      "\n",
      "Bagged predictions shape: (482, 4)\n",
      "Aggregated predictions (y_pred) shape (482,)\n",
      "\n",
      "Bagging RMSE= 0.1116490577989883\n"
     ]
    }
   ],
   "source": [
    "def bagging_predictions(estimator):\n",
    "    \"\"\"\n",
    "    I/P\n",
    "    estimator: The base estimator from which the ensemble is grown.\n",
    "    O/P\n",
    "    br_y_pred: Predictions on test data for the base estimator.\n",
    "    \n",
    "    \"\"\"\n",
    "    regr = BaggingRegressor(base_estimator=estimator,\n",
    "                            n_estimators=10,\n",
    "                            max_samples=1.0,\n",
    "                            bootstrap=True, # Samples are drawn with replacement\n",
    "                            n_jobs= n_jobs,\n",
    "                            random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "    br_y_pred = regr.predict(X_test)\n",
    "\n",
    "    rmse_val = mean_squared_error(y_test, br_y_pred, squared= False) # squared= False > returns Root Mean Square Error   \n",
    "\n",
    "    print(f'RMSE for base estimator {regr.base_estimator_} = {rmse_val}\\n')\n",
    "    return br_y_pred\n",
    "\n",
    "\n",
    "predictions = np.column_stack((bagging_predictions(linear_regression),\n",
    "                              bagging_predictions(lasso),\n",
    "                              bagging_predictions(elastic_net),\n",
    "                              bagging_predictions(kernel_ridge)))\n",
    "print(f\"Bagged predictions shape: {predictions.shape}\")\n",
    "       \n",
    "y_pred = np.mean(predictions, axis=1)\n",
    "print(\"Aggregated predictions (y_pred) shape\", y_pred.shape)\n",
    "\n",
    "rmse_val = mean_squared_error(y_test, y_pred, squared= False) # squared= False > returns Root Mean Square Error   \n",
    "models_scores.append(['Bagging', rmse_val])\n",
    "\n",
    "print(f'\\nBagging RMSE= {rmse_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:29.107411Z",
     "iopub.status.busy": "2021-12-27T07:06:29.106735Z",
     "iopub.status.idle": "2021-12-27T07:06:29.118340Z",
     "shell.execute_reply": "2021-12-27T07:06:29.119035Z"
    },
    "papermill": {
     "duration": 0.043191,
     "end_time": "2021-12-27T07:06:29.119193",
     "exception": false,
     "start_time": "2021-12-27T07:06:29.076002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.111053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.111078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.111649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.131791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KernelRidge</td>\n",
       "      <td>0.136392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1\n",
       "1             Lasso  0.111053\n",
       "2        ElasticNet  0.111078\n",
       "4           Bagging  0.111649\n",
       "0  LinearRegression  0.131791\n",
       "3       KernelRidge  0.136392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking the scores of each model\n",
    "pd.DataFrame(models_scores).sort_values(by=[1], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028173,
     "end_time": "2021-12-27T07:06:29.176008",
     "exception": false,
     "start_time": "2021-12-27T07:06:29.147835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see from above results, because of the high score of \"KernelRidge\" estimator total bagging RMSE is less than that of \"Lasso\" and \"ElasticNet\". \n",
    "\n",
    "### Boosting <a id= \"16\"></a>\n",
    "\n",
    "We are going to use GradientBoostingRegressor, XGBRegressor, LGBMRegressor algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031259,
     "end_time": "2021-12-27T07:06:29.239642",
     "exception": false,
     "start_time": "2021-12-27T07:06:29.208383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### GradientBoostingRegressor <a id= \"17\"></a>\n",
    "\n",
    "* Gradient Boosting for regression.\n",
    "* GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:29.302013Z",
     "iopub.status.busy": "2021-12-27T07:06:29.301310Z",
     "iopub.status.idle": "2021-12-27T07:06:38.231562Z",
     "shell.execute_reply": "2021-12-27T07:06:38.232149Z"
    },
    "papermill": {
     "duration": 8.961987,
     "end_time": "2021-12-27T07:06:38.232292",
     "exception": false,
     "start_time": "2021-12-27T07:06:29.270305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor Score= 0.12087469712016406\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting_regressor= GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state = random_state)\n",
    "\n",
    "score = rmse(gradient_boosting_regressor)\n",
    "models_scores.append(['GradientBoostingRegressor', score])\n",
    "print(f'GradientBoostingRegressor Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030577,
     "end_time": "2021-12-27T07:06:38.294254",
     "exception": false,
     "start_time": "2021-12-27T07:06:38.263677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### XGBRegressor <a id= \"18\"></a>\n",
    "* XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n",
    "* It implements machine learning algorithms under the Gradient Boosting framework.\n",
    "* XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:38.366567Z",
     "iopub.status.busy": "2021-12-27T07:06:38.365882Z",
     "iopub.status.idle": "2021-12-27T07:06:46.085967Z",
     "shell.execute_reply": "2021-12-27T07:06:46.085420Z"
    },
    "papermill": {
     "duration": 7.76146,
     "end_time": "2021-12-27T07:06:46.086087",
     "exception": false,
     "start_time": "2021-12-27T07:06:38.324627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor Score= 0.11566132041864456\n"
     ]
    }
   ],
   "source": [
    "xgb_regressor= xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213,verbosity=0, nthread = -1, random_state = random_state)\n",
    "score = rmse(xgb_regressor)\n",
    "models_scores.append(['XGBRegressor', score])\n",
    "print(f'XGBRegressor Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02941,
     "end_time": "2021-12-27T07:06:46.144763",
     "exception": false,
     "start_time": "2021-12-27T07:06:46.115353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LGBMRegressor <a id= \"19\"></a>\n",
    "Light GBM is a gradient boosting framework that uses tree based learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:46.212570Z",
     "iopub.status.busy": "2021-12-27T07:06:46.211877Z",
     "iopub.status.idle": "2021-12-27T07:06:46.517615Z",
     "shell.execute_reply": "2021-12-27T07:06:46.516863Z"
    },
    "papermill": {
     "duration": 0.343591,
     "end_time": "2021-12-27T07:06:46.517778",
     "exception": false,
     "start_time": "2021-12-27T07:06:46.174187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor Score= 0.118848879537224\n"
     ]
    }
   ],
   "source": [
    "lgbm_regressor= lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11,random_state = random_state)\n",
    "\n",
    "score = rmse(lgbm_regressor)\n",
    "models_scores.append(['LGBMRegressor', score])\n",
    "print(f'LGBMRegressor Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029767,
     "end_time": "2021-12-27T07:06:46.578214",
     "exception": false,
     "start_time": "2021-12-27T07:06:46.548447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Stacking <a id= \"20\"></a>\n",
    "\n",
    "* We can use sklearns **StackingClassifier** and **StackingRegressor** to for classification and regression problem respectively.\n",
    "* Since \"lasso\" is our best performing model we will use it as our meta learner and rest models as base estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:06:46.647059Z",
     "iopub.status.busy": "2021-12-27T07:06:46.646380Z",
     "iopub.status.idle": "2021-12-27T07:07:12.314370Z",
     "shell.execute_reply": "2021-12-27T07:07:12.317930Z"
    },
    "papermill": {
     "duration": 25.70993,
     "end_time": "2021-12-27T07:07:12.318262",
     "exception": false,
     "start_time": "2021-12-27T07:06:46.608332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse= 0.10994668918977511\n"
     ]
    }
   ],
   "source": [
    "estimators = [ ('elastic_net', elastic_net), ('kernel_ridge', kernel_ridge),('xgb_regressor', xgb_regressor) ]\n",
    "\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator= lasso, cv= 5, n_jobs= n_jobs, passthrough = True)\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "pred = stack.predict(X_test)\n",
    "\n",
    "rmse_val = mean_squared_error(y_test, pred, squared= False) # squared= False > returns Root Mean Square Error    \n",
    "models_scores.append(['Stacking', rmse_val])\n",
    "print(f'rmse= {rmse_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-27T07:07:12.411224Z",
     "iopub.status.busy": "2021-12-27T07:07:12.410103Z",
     "iopub.status.idle": "2021-12-27T07:07:12.420500Z",
     "shell.execute_reply": "2021-12-27T07:07:12.421153Z"
    },
    "papermill": {
     "duration": 0.052073,
     "end_time": "2021-12-27T07:07:12.421306",
     "exception": false,
     "start_time": "2021-12-27T07:07:12.369233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stacking</td>\n",
       "      <td>0.109947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.111053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.111078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.111649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.115661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>0.118849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.120875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.131791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KernelRidge</td>\n",
       "      <td>0.136392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0         1\n",
       "8                   Stacking  0.109947\n",
       "1                      Lasso  0.111053\n",
       "2                 ElasticNet  0.111078\n",
       "4                    Bagging  0.111649\n",
       "6               XGBRegressor  0.115661\n",
       "7              LGBMRegressor  0.118849\n",
       "5  GradientBoostingRegressor  0.120875\n",
       "0           LinearRegression  0.131791\n",
       "3                KernelRidge  0.136392"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking the scores of each model\n",
    "pd.DataFrame(models_scores).sort_values(by=[1], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030564,
     "end_time": "2021-12-27T07:07:12.484149",
     "exception": false,
     "start_time": "2021-12-27T07:07:12.453585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see from above list that 'Stacking' resulted in the best possible score. We can even try multiple permutation of each model till we get the best possible results. Apart from Bagging, Boosting and Stacking ensembling methods we can also try model blending approach where we can assign weightage to each model and combining the results till we get the best possible score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 54.453317,
   "end_time": "2021-12-27T07:07:12.628799",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-27T07:06:18.175482",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
