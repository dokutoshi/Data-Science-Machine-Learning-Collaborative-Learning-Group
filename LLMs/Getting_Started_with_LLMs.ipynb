{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A popular resource to learn more about Large Language Models is Hugging Face. [OpenAI](https://openai.com/) is a popular option for building LLM applications. However, if you're beginning your journey, and are looking for a cost effective alternative, then the Hugging Face ecosystem is something worth exploring.\n",
        "\n",
        "\n",
        "### So, what is Hugging Face?\n",
        "\n",
        "It's an open source repository of models, datasets, and tools. It is known for its *Transformers* library, which is designed for natural language processing (NLP) applications.\n",
        "\n",
        "Apart from the Transformers library, there are also thousands of language models freely available to use on [Hugging Face](https://huggingface.co/). The API integrates really nicely into [LangChain](https://www.langchain.com/), which is an open-source framework for connecting LLMs, data sources, and other functionalities under a unified syntax.\n",
        "\n",
        "In this guide, you'll get started with LLMs using Hugging Face.\n",
        "\n",
        "### Steps to access the Hugging Face API token\n",
        "\n",
        "To follow-along, you'll first need to create a Hugging Face API token. Creating this token is completely free, and there are no charges for loading models. The steps are as follows:\n",
        "\n",
        "1. Sign up for a Hugging Face account at https://huggingface.co/join\n",
        "2. Navigate to https://huggingface.co/settings/tokens\n",
        "3. Select `New token` and copy the key\n",
        "\n",
        "Once this is completed, you're ready to get started using LLMs."
      ],
      "metadata": {
        "id": "HuOz3_hgNGbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face models in LangChain\n",
        "\n",
        "To start, install the `langchain_community` module, which contains components you'll need to implement the base interfaces defined in LangChain Core. You can read more [here](https://pypi.org/project/langchain-community/).\n",
        "\n"
      ],
      "metadata": {
        "id": "6AQzIGBwOY5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_community\n",
        "!pip install transformers\n",
        "from langchain_community.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "xMbHeTLEOhvH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step will be to create an object that will store the Hugging Face token we just created."
      ],
      "metadata": {
        "id": "JYpUxGyvO8F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object for your Hugging Face API token\n",
        "huggingfacehub_api_token = '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'"
      ],
      "metadata": {
        "id": "qp_gmxGbO1U4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll define an LLM using the [Falcon-7B instruct model](https://huggingface.co/tiiuae/falcon-7b-instruct) from Hugging Face, which has the ID: `tiiuae/falcon-7b-instruct`."
      ],
      "metadata": {
        "id": "GPevUajjPSpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)"
      ],
      "metadata": {
        "id": "KAXw1GuFPd9n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have instantiated the `llm` object, and we'll pass a text as an input, and then use the `llm.invoke()` function to predict the words based on the input provided. This is shown in the code below."
      ],
      "metadata": {
        "id": "vnVr7No6QLMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'Money is wealth, but is it the most important thing to us?'\n",
        "output = llm.invoke(input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8cQ6kMJQR7h",
        "outputId": "05bd8a64-2bb5-41ec-fc05-386e29712225"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Money is wealth, but is it the most important thing to us?\n",
            "Money is important, but it is not the most important thing to us. Money is important because it allows us to buy the things we need and want, but it is not the most important thing to us. Money is important, but it is not the most important thing to us.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = 'What is better chocolate or ice cream?'\n",
        "output2 = llm.invoke(input2)\n",
        "print(output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8K60foSRUhY",
        "outputId": "575b6abe-cdbe-44c3-b116-dfcccdc76cf3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is better chocolate or ice cream?\n",
            "As an AI language model, I don't have personal preferences. However, both chocolate and ice cream have their unique qualities and benefits. Chocolate is a rich and creamy treat that can be enjoyed in various forms, while ice cream is a frozen dessert that can be made with a variety of flavors. Ultimately, it depends on personal taste and dietary preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the LLM responds different to different inputs. It can be generic one and there may be some repetition. So, there is room for improvement, and it's also a good idea to compare the output using the OpenAI API.\n",
        "\n",
        "However, that's something for the future analysis. Feel free to experiment with other LLMs in Hugging Face."
      ],
      "metadata": {
        "id": "VEGGf19aQ9qs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXoc7IikRRqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}