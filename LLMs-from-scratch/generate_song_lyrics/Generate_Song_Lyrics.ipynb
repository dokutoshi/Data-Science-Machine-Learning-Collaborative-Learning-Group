{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDbG4g8aSZez"
      },
      "source": [
        "## Using a GPT to generate song lyrics\n",
        "This notebook generates song lyrics based on a GPT model and is based on material from [here](https://www.youtube.com/@GPTandChill)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQqKvoi0sN5j"
      },
      "source": [
        "Change Runtime -> Change Runtime Type -> T4 GPU for better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0vw82YZsG6S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1UE_Q_GsYOh"
      },
      "source": [
        "Create the GPT class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji3UY_16sLYy"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\"\n",
        "    A GPT-like transformer model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab_size : int\n",
        "        The size of the vocabulary.\n",
        "    context_length : int\n",
        "        The length of the input context.\n",
        "    model_dim : int\n",
        "        The dimensionality of the model.\n",
        "    num_blocks : int\n",
        "        The number of transformer blocks.\n",
        "    num_heads : int\n",
        "        The number of attention heads.\n",
        "    \"\"\"\n",
        "    class TransformerBlock(nn.Module):\n",
        "        \"\"\"\n",
        "        A single transformer block consisting of multi-headed self-attention\n",
        "        and a feedforward neural network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_dim : int\n",
        "            The dimensionality of the model.\n",
        "        num_heads : int\n",
        "            The number of attention heads.\n",
        "        \"\"\"\n",
        "        class MultiHeadedSelfAttention(nn.Module):\n",
        "            \"\"\"\n",
        "            Multi-headed self-attention mechanism.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            model_dim : int\n",
        "                The dimensionality of the model.\n",
        "            num_heads : int\n",
        "                The number of attention heads.\n",
        "            \"\"\"\n",
        "            class SingleHeadAttention(nn.Module):\n",
        "                \"\"\"\n",
        "                Single head attention mechanism.\n",
        "\n",
        "                Parameters\n",
        "                ----------\n",
        "                model_dim : int\n",
        "                    The dimensionality of the model.\n",
        "                head_size : int\n",
        "                    The size of each attention head.\n",
        "                \"\"\"\n",
        "                def __init__(self, model_dim: int, head_size: int):\n",
        "                    super().__init__()\n",
        "                    self.key_layer = nn.Linear(model_dim, head_size, bias=False)\n",
        "                    self.query_layer = nn.Linear(model_dim, head_size, bias=False)\n",
        "                    self.value_layer = nn.Linear(model_dim, head_size, bias=False)\n",
        "\n",
        "                def forward(self, embedded):\n",
        "                    \"\"\"\n",
        "                    Forward pass for single-head self-attention.\n",
        "\n",
        "                    Parameters\n",
        "                    ----------\n",
        "                    embedded : torch.Tensor\n",
        "                        The input tensor of shape (batch_size, context_length, model_dim).\n",
        "\n",
        "                    Returns\n",
        "                    -------\n",
        "                    torch.Tensor\n",
        "                        The attention-weighted values.\n",
        "                    \"\"\"\n",
        "                    k = self.key_layer(embedded)\n",
        "                    q = self.query_layer(embedded)\n",
        "                    v = self.value_layer(embedded)\n",
        "\n",
        "                    scores = q @ torch.transpose(k, 1, 2)  # Compute attention scores\n",
        "                    context_length, attention_dim = k.shape[1], k.shape[2]\n",
        "                    scores = scores / (attention_dim ** 0.5)  # Scale scores\n",
        "\n",
        "                    # Create a lower triangular mask for causal attention\n",
        "                    lower_triangular = torch.tril(torch.ones(context_length, context_length))\n",
        "                    mask = (lower_triangular == 0).to(device)\n",
        "                    scores = scores.masked_fill(mask, float('-inf'))\n",
        "                    scores = nn.functional.softmax(scores, dim=2)\n",
        "\n",
        "                    return scores @ v  # Weighted sum of values\n",
        "\n",
        "            def __init__(self, model_dim: int, num_heads: int):\n",
        "                super().__init__()\n",
        "                self.attention_heads = nn.ModuleList()\n",
        "                for _ in range(num_heads):\n",
        "                    self.attention_heads.append(self.SingleHeadAttention(model_dim, model_dim // num_heads))\n",
        "                self.compute = nn.Linear(model_dim, model_dim)\n",
        "                self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "            def forward(self, embedded):\n",
        "                \"\"\"\n",
        "                Forward pass for multi-headed self-attention.\n",
        "\n",
        "                Parameters\n",
        "                ----------\n",
        "                embedded : torch.Tensor\n",
        "                    The input tensor of shape (batch_size, context_length, model_dim).\n",
        "\n",
        "                Returns\n",
        "                -------\n",
        "                torch.Tensor\n",
        "                    The output tensor after multi-headed attention.\n",
        "                \"\"\"\n",
        "                head_outputs = [head(embedded) for head in self.attention_heads]\n",
        "                concatenated = torch.cat(head_outputs, dim=2)\n",
        "                return self.dropout(self.compute(concatenated))\n",
        "\n",
        "        class VanillaNeuralNetwork(nn.Module):\n",
        "            \"\"\"\n",
        "            A simple feedforward neural network used within the transformer block.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            model_dim : int\n",
        "                The dimensionality of the model.\n",
        "            \"\"\"\n",
        "            def __init__(self, model_dim: int):\n",
        "                super().__init__()\n",
        "                self.first_linear_layer = nn.Linear(model_dim, model_dim * 4)\n",
        "                self.relu = nn.ReLU()\n",
        "                self.second_linear_layer = nn.Linear(model_dim * 4, model_dim)\n",
        "                self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "            def forward(self, x):\n",
        "                \"\"\"\n",
        "                Forward pass for the feedforward network.\n",
        "\n",
        "                Parameters\n",
        "                ----------\n",
        "                x : torch.Tensor\n",
        "                    Input tensor.\n",
        "\n",
        "                Returns\n",
        "                -------\n",
        "                torch.Tensor\n",
        "                    Output tensor.\n",
        "                \"\"\"\n",
        "                return self.dropout(self.second_linear_layer(self.relu(self.first_linear_layer(x))))\n",
        "\n",
        "        def __init__(self, model_dim: int, num_heads: int):\n",
        "            super().__init__()\n",
        "            self.mhsa = self.MultiHeadedSelfAttention(model_dim, num_heads)\n",
        "            self.vanilla_nn = self.VanillaNeuralNetwork(model_dim)\n",
        "            self.layer_norm_one = nn.LayerNorm(model_dim)\n",
        "            self.layer_norm_two = nn.LayerNorm(model_dim)\n",
        "\n",
        "        def forward(self, embedded):\n",
        "            \"\"\"\n",
        "            Forward pass for the transformer block.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            embedded : torch.Tensor\n",
        "                Input tensor.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            torch.Tensor\n",
        "                Processed tensor.\n",
        "            \"\"\"\n",
        "            embedded = embedded + self.mhsa(self.layer_norm_one(embedded))  # Skip connection\n",
        "            embedded = embedded + self.vanilla_nn(self.layer_norm_two(embedded))  # Another skip connection\n",
        "            return embedded\n",
        "\n",
        "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, model_dim)\n",
        "        self.pos_embedding = nn.Embedding(context_length, model_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[self.TransformerBlock(model_dim, num_heads) for _ in range(num_blocks)])\n",
        "        self.layer_norm_three = nn.LayerNorm(model_dim)\n",
        "        self.vocab_projection = nn.Linear(model_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        \"\"\"\n",
        "        Forward pass for the GPT model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        context : torch.Tensor\n",
        "            Input tensor of token indices.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The logits for the next token prediction.\n",
        "        \"\"\"\n",
        "        embedded = self.token_embedding(context)\n",
        "        context_length = context.shape[1]\n",
        "        positions = torch.arange(context_length).to(device)\n",
        "        embedded = embedded + self.pos_embedding(positions)\n",
        "\n",
        "        raw_output = self.vocab_projection(self.layer_norm_three(self.transformer_blocks(embedded)))\n",
        "        return raw_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Green Day Lyrics generator"
      ],
      "metadata": {
        "id": "oomQmLvLvltt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and process the lyrics data"
      ],
      "metadata": {
        "id": "kp6zSjcJvprh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/GreenDayLyrics.txt', 'r', encoding='utf-8') as f:\n",
        "    lyrics = f.read()"
      ],
      "metadata": {
        "id": "FSOGsDJbvvGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create character-level vocabulary."
      ],
      "metadata": {
        "id": "YDc_trDov0Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = sorted(set(lyrics))\n",
        "char_to_int = {ch: i for i, ch in enumerate(unique_chars)}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "4BXkHqSOv1_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the lyrics data."
      ],
      "metadata": {
        "id": "-XK9yy5Uv2fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_lyrics = [char_to_int[ch] for ch in lyrics]"
      ],
      "metadata": {
        "id": "2xPCBUh9v5aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare input-target sequences."
      ],
      "metadata": {
        "id": "BGtGNx3FwAcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"Generates input-target sequences from a dataset for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : list or numpy.ndarray\n",
        "        Input data from which sequences are generated.\n",
        "    seq_length : int\n",
        "        Length of each input sequence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of torch.Tensor\n",
        "        A tuple (inputs, targets), where:\n",
        "        - inputs: Tensor of shape (num_samples, seq_length) representing input sequences.\n",
        "        - targets: Tensor of shape (num_samples, seq_length) representing target sequences.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Each input sequence consists of `seq_length` consecutive elements from the input data.\n",
        "    - Each target sequence is the corresponding next `seq_length` elements, offset by one.\n",
        "    - Useful for training sequence models like RNNs or Transformers.\n",
        "    \"\"\"\n",
        "    inputs, targets = [], []  # Initialize lists to store input and target sequences.\n",
        "\n",
        "    # Iterate through the data to extract sequences.\n",
        "    for i in range(len(data) - seq_length):\n",
        "        inputs.append(data[i:i + seq_length])  # Input sequence of length `seq_length`.\n",
        "        targets.append(data[i + 1:i + seq_length + 1])  # Corresponding target sequence.\n",
        "\n",
        "    # Convert lists to tensors for use with PyTorch models.\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "seq_length = 128\n",
        "X_train, y_train = create_sequences(encoded_lyrics, seq_length)"
      ],
      "metadata": {
        "id": "eFtCzCHBwAlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f' Number of sequences in training data: {len(X_train)}')"
      ],
      "metadata": {
        "id": "KvefulkxwxTZ",
        "outputId": "dfd93c9f-2935-4550-a3a6-47e1e22413b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Number of sequences in training data: 29744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "MLJxv_aXw2Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = 10_000\n",
        "X_train, y_train = X_train[:subset_size], y_train[:subset_size]\n",
        "\n",
        "# Batch size and training epochs\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = GPT(vocab_size=len(unique_chars), context_length=128, model_dim=252, num_blocks=6, num_heads=6).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AMP API\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# Training loop with batching and mixed precision\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        context = X_train[i:i + batch_size].to(device)\n",
        "        target = y_train[i:i + batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "            output = model(context)\n",
        "            loss = criterion(output.view(-1, len(unique_chars)), target.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / (len(X_train) // batch_size)}\")\n",
        "\n",
        "# Save the improved model\n",
        "torch.save(model.state_dict(), 'green_day_fine_tuned_weights.pt')"
      ],
      "metadata": {
        "id": "zEJe9hgww58X",
        "outputId": "ac284269-2e46-4cb4-d189-50bfeefbb437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.9024766439046616\n",
            "Epoch 2, Loss: 2.6327119515492368\n",
            "Epoch 3, Loss: 2.5627492788510446\n",
            "Epoch 4, Loss: 2.5151719863598165\n",
            "Epoch 5, Loss: 2.4764270048875074\n",
            "Epoch 6, Loss: 2.4380006912427072\n",
            "Epoch 7, Loss: 2.3940151868722377\n",
            "Epoch 8, Loss: 2.346079939450973\n",
            "Epoch 9, Loss: 2.2829126241879587\n",
            "Epoch 10, Loss: 2.20493516096702\n",
            "Epoch 11, Loss: 2.1052387906954837\n",
            "Epoch 12, Loss: 1.9831893016130497\n",
            "Epoch 13, Loss: 1.8581082668059912\n",
            "Epoch 14, Loss: 1.7194860638716283\n",
            "Epoch 15, Loss: 1.5789378713338802\n",
            "Epoch 16, Loss: 1.4317210461848822\n",
            "Epoch 17, Loss: 1.2891977261274288\n",
            "Epoch 18, Loss: 1.164544566319539\n",
            "Epoch 19, Loss: 1.0577828387419383\n",
            "Epoch 20, Loss: 0.9674465885529151\n",
            "Epoch 21, Loss: 0.8631298137016785\n",
            "Epoch 22, Loss: 0.7554733126591413\n",
            "Epoch 23, Loss: 0.6698395644242947\n",
            "Epoch 24, Loss: 0.5768747604810275\n",
            "Epoch 25, Loss: 0.4988027130946135\n",
            "Epoch 26, Loss: 0.43181255555305725\n",
            "Epoch 27, Loss: 0.37963918004280484\n",
            "Epoch 28, Loss: 0.3327513058216144\n",
            "Epoch 29, Loss: 0.2981765356201392\n",
            "Epoch 30, Loss: 0.2632058920004429\n",
            "Epoch 31, Loss: 0.24786410452081606\n",
            "Epoch 32, Loss: 0.21632216670192206\n",
            "Epoch 33, Loss: 0.19303922068614227\n",
            "Epoch 34, Loss: 0.17835589383657163\n",
            "Epoch 35, Loss: 0.16956284909676284\n",
            "Epoch 36, Loss: 0.1555490656158863\n",
            "Epoch 37, Loss: 0.1454444388166452\n",
            "Epoch 38, Loss: 0.1423024408137187\n",
            "Epoch 39, Loss: 0.12826263522490478\n",
            "Epoch 40, Loss: 0.1212383751781323\n",
            "Epoch 41, Loss: 0.11653601846251732\n",
            "Epoch 42, Loss: 0.11338359246460292\n",
            "Epoch 43, Loss: 0.10791366408841732\n",
            "Epoch 44, Loss: 0.10547518797027759\n",
            "Epoch 45, Loss: 0.10150261447788814\n",
            "Epoch 46, Loss: 0.09909279314944378\n",
            "Epoch 47, Loss: 0.09709232214551705\n",
            "Epoch 48, Loss: 0.09391159487840457\n",
            "Epoch 49, Loss: 0.0914949587522409\n",
            "Epoch 50, Loss: 0.09135557558291997\n",
            "Epoch 51, Loss: 0.09038675786593021\n",
            "Epoch 52, Loss: 0.08970726653933525\n",
            "Epoch 53, Loss: 0.08792740531647816\n",
            "Epoch 54, Loss: 0.08635907968840538\n",
            "Epoch 55, Loss: 0.08471869371640377\n",
            "Epoch 56, Loss: 0.08193940984515044\n",
            "Epoch 57, Loss: 0.08080304318513626\n",
            "Epoch 58, Loss: 0.08199855775978321\n",
            "Epoch 59, Loss: 0.08078241930940212\n",
            "Epoch 60, Loss: 0.08098053870101769\n",
            "Epoch 61, Loss: 0.07898550743284898\n",
            "Epoch 62, Loss: 0.07742773349850605\n",
            "Epoch 63, Loss: 0.07701884429806317\n",
            "Epoch 64, Loss: 0.09823666938031331\n",
            "Epoch 65, Loss: 0.1364771597660505\n",
            "Epoch 66, Loss: 0.1867530109026493\n",
            "Epoch 67, Loss: 0.17538087996534812\n",
            "Epoch 68, Loss: 0.12966244295239449\n",
            "Epoch 69, Loss: 0.09923589129287463\n",
            "Epoch 70, Loss: 0.08354458337028821\n",
            "Epoch 71, Loss: 0.07666166336872639\n",
            "Epoch 72, Loss: 0.07373963043284722\n",
            "Epoch 73, Loss: 0.07111392829280633\n",
            "Epoch 74, Loss: 0.06961185752581327\n",
            "Epoch 75, Loss: 0.07056640394223042\n",
            "Epoch 76, Loss: 0.07078918991371608\n",
            "Epoch 77, Loss: 0.06918622534244488\n",
            "Epoch 78, Loss: 0.06876443097224602\n",
            "Epoch 79, Loss: 0.06738316797866271\n",
            "Epoch 80, Loss: 0.06744767725467682\n",
            "Epoch 81, Loss: 0.06672841458557507\n",
            "Epoch 82, Loss: 0.06790862700495964\n",
            "Epoch 83, Loss: 0.06677991271210022\n",
            "Epoch 84, Loss: 0.06716729251620097\n",
            "Epoch 85, Loss: 0.06925468013072625\n",
            "Epoch 86, Loss: 0.06686661650355046\n",
            "Epoch 87, Loss: 0.06582726643253596\n",
            "Epoch 88, Loss: 0.06539936932042623\n",
            "Epoch 89, Loss: 0.0651591602139748\n",
            "Epoch 90, Loss: 0.0655769998064408\n",
            "Epoch 91, Loss: 0.064712285852203\n",
            "Epoch 92, Loss: 0.06619116773781104\n",
            "Epoch 93, Loss: 0.06527485555181137\n",
            "Epoch 94, Loss: 0.06503903369108836\n",
            "Epoch 95, Loss: 0.06485166969016576\n",
            "Epoch 96, Loss: 0.06484915483265351\n",
            "Epoch 97, Loss: 0.06376898689911915\n",
            "Epoch 98, Loss: 0.06350778735792026\n",
            "Epoch 99, Loss: 0.0631775230837938\n",
            "Epoch 100, Loss: 0.06447241603372952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to generate lyrics."
      ],
      "metadata": {
        "id": "DTO8hXoHxK5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics(model, new_chars, context, context_length, int_to_char, temperature=1.0):\n",
        "    \"\"\"Generates lyrics using a trained character-level language model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        The trained language model for character generation.\n",
        "    new_chars : int\n",
        "        Number of new characters to generate.\n",
        "    context : torch.Tensor\n",
        "        Input tensor representing the initial context (shape: [1, sequence_length]).\n",
        "    context_length : int\n",
        "        Maximum length of context to retain during generation.\n",
        "    int_to_char : dict\n",
        "        Mapping from integer indices to characters.\n",
        "    temperature : float, optional\n",
        "        Sampling temperature controlling randomness. Lower values make predictions more deterministic,\n",
        "        while higher values increase diversity. Default is 1.0.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Generated lyrics as a string.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - The function performs autoregressive generation by sampling one character at a time.\n",
        "    - Uses softmax with a temperature parameter to control the randomness of predictions.\n",
        "    - Context is truncated to `context_length` to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.).\n",
        "    res = []  # Store generated characters.\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for faster inference.\n",
        "        for _ in range(new_chars):\n",
        "            # Keep only the last `context_length` characters.\n",
        "            if context.shape[1] > context_length:\n",
        "                context = context[:, -context_length:]\n",
        "\n",
        "            # Forward pass: generate model output (logits).\n",
        "            output = model(context)\n",
        "\n",
        "            # Extract logits for the last time step and apply temperature scaling.\n",
        "            logits = output[:, -1, :] / temperature\n",
        "\n",
        "            # Convert logits to probabilities using softmax.\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample the next character index from the probability distribution.\n",
        "            next_char = torch.multinomial(probs, 1)\n",
        "\n",
        "            # Append the new character to the context.\n",
        "            context = torch.cat((context, next_char), dim=-1)\n",
        "\n",
        "            # Map the character index to its corresponding character and store it.\n",
        "            res.append(int_to_char[next_char.item()])\n",
        "\n",
        "    return ''.join(res)  # Return the generated lyrics as a string."
      ],
      "metadata": {
        "id": "xw-CZHRjxLwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate lyrics for new Green Day songs."
      ],
      "metadata": {
        "id": "PEI0ET3hxRwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I walk alone\"\n",
        "start_context = torch.tensor([[char_to_int[c] for c in seed_text]], dtype=torch.int64).to(device)\n",
        "\n",
        "new_lyrics = generate_lyrics(model, new_chars=2000, context=start_context, context_length=128, int_to_char=int_to_char, temperature=0.9)\n",
        "print(new_lyrics)"
      ],
      "metadata": {
        "id": "pHeMIGTYxTZd",
        "outputId": "4e5d4b86-ace7-4643-e68e-fe5a2390de59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I walk a\n",
            "\n",
            "I walk this empty street\n",
            "On the Boulevard of Broken Dreams\n",
            "Where the city sleeps\n",
            "And I'm the only one and I walk a\n",
            "\n",
            "My shadow's the only one that walks beside me\n",
            "My shallow heart's the only thing that's beating\n",
            "Sometimes I wish someone out there will find me\n",
            "'Til then I walk alone\n",
            "\n",
            "Ah-ah, ah-ah, aaah-ah\n",
            "Ah-ah, ah-ah\n",
            "\n",
            "I walk a\n",
            "\n",
            "I walk a\n",
            "\n",
            "I walk this empty street\n",
            "On the Boulevard of Broken Dreams\n",
            "Where the city sleeps\n",
            "And I'm the only one and I walk a\n",
            "\n",
            "My shadow's the only one that walks beside me\n",
            "My shallow heart's the only thing that's beating\n",
            "Sometimes I wish someone out there will find me\n",
            "'Til then I walk alone\n",
            "\n",
            "Ah-ah, ah-ah, ah-ah, aaah-ah\n",
            "Ah-ah, ah-ah\n",
            "\n",
            "I walk a\n",
            "\n",
            "I walk this empty street\n",
            "On the Boulevard of Broken Dreams\n",
            "Where the city sleeps\n",
            "And I'm the only one and I walk alone\n",
            "\n",
            "I walk alone\n",
            "I walk a\n",
            "\n",
            "My shadow's the only one that walks beside me\n",
            "My shallow heart's the only thing that's beating\n",
            "Sometimes I wish someone out there will find me\n",
            "'Til then I walk alone \n",
            "\n",
            "Don't wanna be an American idiot\n",
            "Don't want a nation under the new media\n",
            "And can you hear the sound of hysteria?\n",
            "The subliminal mind fuck America\n",
            "\n",
            "Welcome to a new kind of tension\n",
            "All across the alienation\n",
            "Where everything isn't meant to be okay\n",
            "Television dreams of tomorrow\n",
            "We're not the ones who're meant to follow\n",
            "For that's enough to argue\n",
            "\n",
            "Do you have the time\n",
            "To listen to me whine\n",
            "About nothing and everything\n",
            "All at once\n",
            "I am one of those\n",
            "Melodramatic fools\n",
            "Neurotic to the bone\n",
            "No doubt about it\n",
            "\n",
            "Sometimes I give myself the creeps\n",
            "Sometimes my mind plays tricks on me\n",
            "It all keeps adding up\n",
            "I think I'm cracking up\n",
            "Am I just paranoid?\n",
            "Uh, yuh, ya\n",
            "\n",
            "Grasping to control\n",
            "So I better hold on\n",
            "\n",
            "Sometimes I give myself the creeps\n",
            "Sometimes my mind plays tricks on me\n",
            "It all keeps adding up\n",
            "I think I'm cracking up\n",
            "Am I just paranoid?\n",
            "Uh, yuh, ya\n",
            "\n",
            "Grasping to control\n",
            "So I better hold on\n",
            "\n",
            "Sometimes I give myself the creeps\n",
            "Sometimes my mind plays tricks on me\n",
            "It all keeps adding up\n",
            "I think I'm cr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"A throbbing tumor and a radiation high\"\n",
        "start_context = torch.tensor([[char_to_int[c] for c in seed_text]], dtype=torch.int64).to(device)\n",
        "\n",
        "new_lyrics = generate_lyrics(model, new_chars=4000, context=start_context, context_length=128, int_to_char=int_to_char, temperature=1.2)\n",
        "print(new_lyrics)"
      ],
      "metadata": {
        "id": "muLT6iOLxcwY",
        "outputId": "2ade4455-9664-4ea5-91d9-f36e013b085c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t\n",
            "One my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go \n",
            "\n",
            "I text a postcard sent to you\n",
            "Did it go through?\n",
            "Sending all my love to you\n",
            "You are the moonlight of my life\n",
            "Every night\n",
            "Giving all my love to you\n",
            "\n",
            "My beating heart belongs to you\n",
            "I walked for miles 'til I found you\n",
            "I'm here to honor you\n",
            "If I lose everything in the fire\n",
            "I'm sending something new\n",
            "\n",
            "I'm having trouble trying to sleep\n",
            "I'm counting sheep but running out\n",
            "As time ticks by\n",
            "Still I try\n",
            "No rest for cross-tops in my mind\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go \n",
            "\n",
            "I text a postcard sent to you\n",
            "Did it go through?\n",
            "Sending all my love to you\n",
            "You are the moonlight of my life\n",
            "Every night\n",
            "Giving all my love to you\n",
            "\n",
            "My beating heart belongs to you\n",
            "I walked for miles 'til I found you\n",
            "I'm here to honor you\n",
            "If I lose everything in the fire\n",
            "I'm sending something new\n",
            "\n",
            "I'm having trouble trying to sleep\n",
            "I'm counting sheep but running out\n",
            "As time ticks by\n",
            "Still I try\n",
            "No rest for cross-tops in my mind\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go \n",
            "\n",
            "I text a postcard sent to you\n",
            "Did it go through?\n",
            "Sending all my love to you\n",
            "You are the moonlight of my life\n",
            "Every night\n",
            "Giving all my love to you\n",
            "\n",
            "My beating heart belongs to you\n",
            "I walked for miles 'til I found you\n",
            "I'm here to honor you\n",
            "If I lefoughts everything in the fire\n",
            "I'm sending something new\n",
            "\n",
            "I'm having trouble trying to sleep\n",
            "I'm counting sheep but running out\n",
            "As time ticks by\n",
            "Still I try\n",
            "No rest for cross-tops in my mind\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go \n",
            "\n",
            "I text a postcard sent to you\n",
            "Did it go through?\n",
            "Sending all my love to you\n",
            "You are the moonlight of my life\n",
            "Every night\n",
            "Giving all my love to you\n",
            "\n",
            "My beating heart belongs to you\n",
            "I walked for miles 'til I found you\n",
            "I'm here to honor you\n",
            "If I lose everything in the fire\n",
            "I'm sending something new\n",
            "\n",
            "I'm having trouble trying to sleep\n",
            "I'm counting sheep but running out\n",
            "As time ticks by\n",
            "Still I try\n",
            "No rest for cross-tops in my mind\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go\n",
            "\n",
            "My mind is set on overdrive\n",
            "The clock is laughing in my face\n",
            "A crooked spine\n",
            "My senses dulled\n",
            "Past the point of delirium\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go\n",
            "\n",
            "My mind is set on overdrive\n",
            "The clock is laughing in my face\n",
            "A crooked spine\n",
            "My senses dulled\n",
            "Past the point of delirium\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go \n",
            "\n",
            "I text a postcard sent to you\n",
            "Did it go through?\n",
            "Sending all my love to you\n",
            "You are the moonlight of my life\n",
            "Every night\n",
            "Giving all my love to you\n",
            "\n",
            "My beating heart belongs to you\n",
            "I walked for miles 'til I found you\n",
            "I'm here to honor you\n",
            "If I lose everything in the fire\n",
            "I'm sending something new\n",
            "\n",
            "I'm having trouble trying to sleep\n",
            "I'm counting sheep but running out\n",
            "As time ticks by\n",
            "Still I try\n",
            "No rest for cross-tops in my mind\n",
            "On my own... here we go\n",
            "\n",
            "My eyes feel like they're gonna bleed\n",
            "Dried up and bulging out my skull\n",
            "My mouth is dry\n",
            "My face is numb\n",
            "Fucked up and spun out in my room\n",
            "On my own... here we go \n",
            "\n",
            "I text a postcard sent to you\n",
            "Did it go through?\n",
            "Sending all my love to you\n",
            "You are t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taylor Swift Lyrics Generator"
      ],
      "metadata": {
        "id": "JZA696M4vsLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and process the lyrics data."
      ],
      "metadata": {
        "id": "jakm4OKxLrwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/TaylorLyrics.txt', 'r', encoding='utf-8') as f:\n",
        "    lyrics = f.read()"
      ],
      "metadata": {
        "id": "PbVM6tFdLm4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create character-level vocabulary."
      ],
      "metadata": {
        "id": "IHilimZmPYnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = sorted(set(lyrics))\n",
        "char_to_int = {ch: i for i, ch in enumerate(unique_chars)}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "d22akECHPYCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the lyrics data."
      ],
      "metadata": {
        "id": "1lahpG9NPc81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_lyrics = [char_to_int[ch] for ch in lyrics]"
      ],
      "metadata": {
        "id": "K-kC61c3PfAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare input-target sequences."
      ],
      "metadata": {
        "id": "GurhJ-UbPghf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 128\n",
        "X_train, y_train = create_sequences(encoded_lyrics, seq_length)"
      ],
      "metadata": {
        "id": "CGPIwgKYPhzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f' Number of sequences in training data: {len(X_train)}')"
      ],
      "metadata": {
        "id": "H-07kKQPZjf6",
        "outputId": "53698ab2-123d-42c9-9dfb-80e24aebc94f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Number of sequences in training data: 278642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "y5LJ9z_bPnwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = 10_000\n",
        "X_train, y_train = X_train[:subset_size], y_train[:subset_size]\n",
        "\n",
        "# Batch size and training epochs\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = GPT(vocab_size=len(unique_chars), context_length=128, model_dim=252, num_blocks=6, num_heads=6).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AMP API\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# Training loop with batching and mixed precision\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        context = X_train[i:i + batch_size].to(device)\n",
        "        target = y_train[i:i + batch_size].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "            output = model(context)\n",
        "            loss = criterion(output.view(-1, len(unique_chars)), target.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / (len(X_train) // batch_size)}\")\n",
        "\n",
        "# Save the improved model\n",
        "torch.save(model.state_dict(), 'taylor_swift_tuned_weights.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBxUcI6fPoT_",
        "outputId": "a26e34f1-5ced-478b-d598-d72e6e94e90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.8535317243673863\n",
            "Epoch 2, Loss: 2.522774983675052\n",
            "Epoch 3, Loss: 2.461463885429578\n",
            "Epoch 4, Loss: 2.4210038368518534\n",
            "Epoch 5, Loss: 2.388933756412604\n",
            "Epoch 6, Loss: 2.3499836371495175\n",
            "Epoch 7, Loss: 2.29807387254177\n",
            "Epoch 8, Loss: 2.2212006052335105\n",
            "Epoch 9, Loss: 2.116103111169277\n",
            "Epoch 10, Loss: 1.986960226144546\n",
            "Epoch 11, Loss: 1.8597185061528132\n",
            "Epoch 12, Loss: 1.7331275618993318\n",
            "Epoch 13, Loss: 1.6040427852899601\n",
            "Epoch 14, Loss: 1.4705819158982008\n",
            "Epoch 15, Loss: 1.3372104045672295\n",
            "Epoch 16, Loss: 1.2111956568864675\n",
            "Epoch 17, Loss: 1.0967308642008367\n",
            "Epoch 18, Loss: 0.9920510183542203\n",
            "Epoch 19, Loss: 0.8839907928919181\n",
            "Epoch 20, Loss: 0.7812798684224104\n",
            "Epoch 21, Loss: 0.6896649396572357\n",
            "Epoch 22, Loss: 0.6089869145399485\n",
            "Epoch 23, Loss: 0.5374672458722041\n",
            "Epoch 24, Loss: 0.47477339857663864\n",
            "Epoch 25, Loss: 0.4162102102851256\n",
            "Epoch 26, Loss: 0.3610234289215161\n",
            "Epoch 27, Loss: 0.3221076520589682\n",
            "Epoch 28, Loss: 0.2878520399905168\n",
            "Epoch 29, Loss: 0.2557751651948843\n",
            "Epoch 30, Loss: 0.2288477515371946\n",
            "Epoch 31, Loss: 0.20189154864503786\n",
            "Epoch 32, Loss: 0.18191434853734115\n",
            "Epoch 33, Loss: 0.167844344694645\n",
            "Epoch 34, Loss: 0.1574008780030104\n",
            "Epoch 35, Loss: 0.1481462115278611\n",
            "Epoch 36, Loss: 0.1383842989229239\n",
            "Epoch 37, Loss: 0.12982549078953573\n",
            "Epoch 38, Loss: 0.12412036095674221\n",
            "Epoch 39, Loss: 0.11720708872263248\n",
            "Epoch 40, Loss: 0.11172050400040089\n",
            "Epoch 41, Loss: 0.1068282584922436\n",
            "Epoch 42, Loss: 0.10567175873961204\n",
            "Epoch 43, Loss: 0.10008468808462986\n",
            "Epoch 44, Loss: 0.09723986627963874\n",
            "Epoch 45, Loss: 0.0945061876032597\n",
            "Epoch 46, Loss: 0.09293237027640526\n",
            "Epoch 47, Loss: 0.08982525794551922\n",
            "Epoch 48, Loss: 0.08850551133927627\n",
            "Epoch 49, Loss: 0.08714640417542213\n",
            "Epoch 50, Loss: 0.08560558599539292\n",
            "Epoch 51, Loss: 0.08447914088192658\n",
            "Epoch 52, Loss: 0.0845130772735828\n",
            "Epoch 53, Loss: 0.08499893073279124\n",
            "Epoch 54, Loss: 0.08421080239499226\n",
            "Epoch 55, Loss: 0.08502838846582633\n",
            "Epoch 56, Loss: 0.0854721183482653\n",
            "Epoch 57, Loss: 0.0837299434038309\n",
            "Epoch 58, Loss: 0.08137121161398216\n",
            "Epoch 59, Loss: 0.07866629060262288\n",
            "Epoch 60, Loss: 0.07650115192891696\n",
            "Epoch 61, Loss: 0.07610684012373288\n",
            "Epoch 62, Loss: 0.07669853743834373\n",
            "Epoch 63, Loss: 0.07625626782194161\n",
            "Epoch 64, Loss: 0.07754721525961007\n",
            "Epoch 65, Loss: 0.07590761971779358\n",
            "Epoch 66, Loss: 0.07453664769537938\n",
            "Epoch 67, Loss: 0.07582914318220738\n",
            "Epoch 68, Loss: 0.08208655054943684\n",
            "Epoch 69, Loss: 0.09403839998711379\n",
            "Epoch 70, Loss: 0.10854909803049687\n",
            "Epoch 71, Loss: 0.17008945087973887\n",
            "Epoch 72, Loss: 0.1815465817657801\n",
            "Epoch 73, Loss: 0.12344491381484729\n",
            "Epoch 74, Loss: 0.09119928914767045\n",
            "Epoch 75, Loss: 0.07839487397517914\n",
            "Epoch 76, Loss: 0.07134696702735546\n",
            "Epoch 77, Loss: 0.07102340225798962\n",
            "Epoch 78, Loss: 0.07110830955207348\n",
            "Epoch 79, Loss: 0.06885291621662103\n",
            "Epoch 80, Loss: 0.06893949578396785\n",
            "Epoch 81, Loss: 0.06796090677380562\n",
            "Epoch 82, Loss: 0.06696617880310768\n",
            "Epoch 83, Loss: 0.06541273800226358\n",
            "Epoch 84, Loss: 0.06434043329686691\n",
            "Epoch 85, Loss: 0.06414085459441711\n",
            "Epoch 86, Loss: 0.06438668339680402\n",
            "Epoch 87, Loss: 0.06458364847378853\n",
            "Epoch 88, Loss: 0.06336338244951688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate lyrics for new Taylor Swift songs."
      ],
      "metadata": {
        "id": "GGb7puw3Pw_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Out of the Woods\"\n",
        "start_context = torch.tensor([[char_to_int[c] for c in seed_text]], dtype=torch.int64).to(device)\n",
        "\n",
        "new_lyrics = generate_lyrics(model, new_chars=2000, context=start_context, context_length=128, int_to_char=int_to_char, temperature=1.0)\n",
        "print(new_lyrics)"
      ],
      "metadata": {
        "id": "NGvPU1EDP04K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26436668-0f47-483e-ed75-1427715ca66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " find I wishing star\n",
            "He's the song in the car I keep singing. Don't know why I do\n",
            "\n",
            "So, I drive home alone\n",
            "As I turn out the light\n",
            "I'll put his picture down\n",
            "And maybe get some sleep tonight\n",
            "\n",
            "'Cause he's the reason for the teardrops on my guitar\n",
            "The only one who's got enough of me to break my heart\n",
            "He's the song in the car I keep singing. Don't know why I do\n",
            "\n",
            "He's the time taken up, but there's never enough\n",
            "And he's all that I need to fall into\n",
            "\n",
            "Drew looks at me\n",
            "I fake a smile so he won't see\n",
            "\n",
            "I don't know what I want, so don't ask me\n",
            "Cause I'm still trying to figure ind a place in this world\n",
            "\n",
            "Got the radio on, my old blue jeans\n",
            "And I'm wearing my heart on my sleeve\n",
            "Feeling lucky today, got the sunshine\n",
            "Could you tell me what more do I need\n",
            "And tomorrow's just a mystery, oh yeah\n",
            "But that's ok\n",
            "\n",
            "[Chorus:]\n",
            "\n",
            "Maybe I'm just a girl on a mission\n",
            "But I'm ready to fly\n",
            "\n",
            "I'm alone, on my own, and that's all I know\n",
            "I'll be strong, I'll be wrong, oh but life goes on\n",
            "Oh I'm alone, on my own, and that's all I know\n",
            "Oh I'm just a girl, trying to find a place in this world\n",
            "\n",
            "Oh I'm just a girl\n",
            "\n",
            "You have a way of coming easily to me\n",
            "And when you take, you take the very best of me\n",
            "So I start a fight 'cause I need to feel something\n",
            "And you do what you want 'cause I'm not what you wanted\n",
            "\n",
            "Oh, what a shame, what a rainy ending given to a perfect day\n",
            "Just walk away, no use defending words that you will never say\n",
            "And now that I way, saydriver it a day\n",
            "And now that I'm sitting here thinking it through\n",
            "I've never been anywhere cold as you\n",
            "\n",
            "You never did give a damn thing, honey, but I cried, cried for you\n",
            "And I know you wouldn't have told nobody if I died, died for you, died for you\n",
            "\n",
            "Oh, what a shame, what a rainy ending given to a perfect day\n",
            "Just walk away, no use defending words that you will never say\n",
            "And now that I way, say\n",
            "\n",
            "[Chorus:]\n",
            "I'm alone, on my own, and that's all I know\n",
            "I'll be strong, I'll be wrong, oh but life goes on\n",
            "Oh, I'm just a girl, trying to find a place in this world\n",
            "\n",
            "Got \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"In the middle of town\"\n",
        "start_context = torch.tensor([[char_to_int[c] for c in seed_text]], dtype=torch.int64).to(device)\n",
        "\n",
        "new_lyrics = generate_lyrics(model, new_chars=4000, context=start_context, context_length=128, int_to_char=int_to_char, temperature=0.8)\n",
        "print(new_lyrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udFbHuHW9RoW",
        "outputId": "ea5a5ec9-b83e-4932-d9f1-ff495fcf5537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I'm standin' on your street\n",
            "And there's a letter left on your doorstep\n",
            "And the first few times\n",
            "I right?\n",
            "\n",
            "So how can I ever try to be better?\n",
            "Nobody ever lets me in\n",
            "I can still see you.\n",
            "This ain't the best view\n",
            "On the outside looking in\n",
            "And I've been a lot of lonely places\n",
            "I've never been on the outside\n",
            "\n",
            "Seems the only one who doesn't see your beauty\n",
            "Is the face in the mirror looking back at you\n",
            "You walk around here thinking you're not pretty\n",
            "But that's not true 'cause I know you\n",
            "\n",
            "Hold on, baby, you're losing it\n",
            "The water's high, you're jumping into it\n",
            "And letting go\n",
            "And no one knows\n",
            "That you cry, but you don't tell anyone\n",
            "That you might not be the golden one\n",
            "And you're tied together with a smile\n",
            "But you're coming undone\n",
            "\n",
            "You're tied together with a smile\n",
            "But you're coming undone\n",
            "Goodbye, baby\n",
            "With a smile, baby, baby\n",
            "\n",
            "Cory's eyes are like a jungle\n",
            "He smiles, it's like the radio\n",
            "He whispers songs into my window\n",
            "In words that nobody knows\n",
            "There's pretty girls on every corner\n",
            "That watch him as he's walking home\n",
            "Saying, \"Does he know?\"\n",
            "Will you ever know?\n",
            "\n",
            "[Chorus:]\n",
            "You're beautiful\n",
            "Every little piece love\n",
            "And don't you know\n",
            "You're really gonna be someone\n",
            "Ask anyone\n",
            "And when you find everything you looked for\n",
            "I hope your life leads you back to my door\n",
            "Oh, but if it don't\n",
            "Stay beautiful\n",
            "\n",
            "Cory finds another way to be\n",
            "The highlight of my day\n",
            "I'm taking pictures in my mind\n",
            "So I can save them for a rainy day\n",
            "It's hard to make a conversation\n",
            "When he's taking my breath away\n",
            "I should say, hey, by the way\n",
            "\n",
            "[Chorus]\n",
            "\n",
            "If you and I are a story\n",
            "That never gets told\n",
            "If what you are is a daydream\n",
            "I'll never get to hold\n",
            "At least you'll know\n",
            "\n",
            "You're goes at I wall to\n",
            "\n",
            "Hold be there, baby\n",
            "\n",
            "If way, by, but nothinever say\n",
            "And now that I way I do\n",
            "\n",
            "Hold on, baby, you're losing it\n",
            "The water's high, you're jumping into it\n",
            "And letting go\n",
            "And no one knows\n",
            "That you cry, but you don't tell anyone\n",
            "That you might not be the golden one\n",
            "And you're tied together with a smile\n",
            "But you're coming undone\n",
            "\n",
            "Hold on, baby, you're losing it\n",
            "The water's high, you're jumping into it\n",
            "And letting go\n",
            "And no one knows\n",
            "That you cry, but you don't tell anyone\n",
            "That you might not be the golden one\n",
            "And you're tied together with a smile\n",
            "But you're coming undone\n",
            "\n",
            "You're tied together with a smile\n",
            "But you're coming undone\n",
            "Goodbye, baby\n",
            "With a smile, baby, baby\n",
            "\n",
            "Cory's eyes are like a jungle\n",
            "He smiles, it's like the radio\n",
            "He whispers songs into my window\n",
            "In words that nobody knows\n",
            "There's pretty girls on every corner\n",
            "That watch him as he's walking home\n",
            "Saying, \"Does he know?\"\n",
            "Will you ever know?\n",
            "\n",
            "[Chorus:]\n",
            "You're beautiful\n",
            "Every little piece love\n",
            "And don't you know\n",
            "You're really gonna be someone\n",
            "Ask anyone\n",
            "And when you find everything you looked for\n",
            "I hope your life leads you back to my door\n",
            "Oh, but if it don't\n",
            "Stay beautiful\n",
            "\n",
            "Cory finds another way to be\n",
            "The highlight of my day\n",
            "I'm taking pictures in my mind\n",
            "So I can save them for a rainy day\n",
            "It's hard to make a conversation\n",
            "When he's taking my breath away\n",
            "I should say, hey, by the way\n",
            "\n",
            "[Chorus]\n",
            "\n",
            "If you and I are a story\n",
            "That never gets told\n",
            "If what you are is a daydream\n",
            "I'll never get to hold\n",
            "At least you'll know\n",
            "\n",
            "You're just a girl it's not to fine to a place in this world\n",
            "\n",
            "Got the radio on, my old blue jeans\n",
            "And I'm wearing my heart on my sleeve\n",
            "Feeling lucky today, got the sunshine\n",
            "Could you tell me what more do I need\n",
            "And tomorrow's just a mystery, oh yeah\n",
            "But that's ok\n",
            "\n",
            "[Chorus:]\n",
            "\n",
            "Maybe I'm just a girl on a mission\n",
            "But I'm ready to fly\n",
            "\n",
            "I'm alone, on my own, and that's all I know\n",
            "Oh I'm just a girl\n",
            "\n",
            "You have a way of coming easily to me\n",
            "And when you take, you take the very best of me\n",
            "So I start a fight 'cause I need to feel something\n",
            "And you do what you want 'cause I'm not what you wanted\n",
            "\n",
            "Oh, what a shame, what a rainy ending given to a perfect day\n",
            "Every smile you fake is so condescending all the scars you made\n",
            "And now that I'm sitting here thinking it through\n",
            "I've never been anywhere cold as you\n",
            "\n",
            "You never did give a damn thin\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}