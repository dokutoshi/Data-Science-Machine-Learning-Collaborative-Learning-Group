{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSpGT_cTYMxV"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and a lot of edits made by Matthew Kehoe.\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxUaTrMFYMxW"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "You've almost reached the end of this book. This last chapter will summarize and review core concepts while also expanding your horizons beyond what you've learned so far. Becoming an effective AI practitioner is a journey, and finishing this book is merely your first step on it. I want to make sure you realize this and are properly equipped to take the next steps of this journey on your own.\n",
        "\n",
        "We'll start with a bird's-eye view of what you should take away from this book. This should refresh your memory regarding some of the concepts you've learned. Next, I'll present an overview of some key limitations of deep learning. To use a tool appropriately, you should not only understand what it **can** do but also be aware of what it **can't** do. Finally, I'll offer some speculative thoughts about the future evolution of deep learning, machine learning, and AI. This should be especially interesting to you if you'd like to get into fundamental research. The chapter\n",
        "ends with a short list of resources and strategies for further learning about machine learning and staying up to date with new advances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MLy7FOMYMxX"
      },
      "source": [
        "## Key concepts in review"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section briefly synthesizes key takeaways from this book. If you ever need a quick refresher to help you recall what you've learned, you can read these few pages."
      ],
      "metadata": {
        "id": "MxFhqi9HiV_G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqn1RVLAYMxX"
      },
      "source": [
        "### Various approaches to AI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, deep learning isn't synonymous with AI, or even with machine learning:\n",
        "\n",
        "- **Artificial intelligence** (AI) is an ancient, broad field that can generally be understood as “all attempts to automate human cognitive processes.” This can range from the very basic, such as an Excel spreadsheet, to the very advanced, like a humanoid robot that can walk and talk.\n",
        "- **Machine learning** is a specific subfield of AI that aims at automatically developing programs (called **models**) purely from exposure to training data. This process of turning data into a program is called **learning**. Although machine learning has been around for a long time, it only started to take off in the 1990s, before becoming the dominant form of AI in the 2000s.\n",
        "- **Deep learning** is one of many branches of machine learning, where the models are long chains of geometric transformations, applied one after the other. These operations are structured into modules called **layers**: deep learning models are typically stacks of layers—or, more generally, graphs of layers. These layers are parameterized by **weights**, which are the parameters learned during training. The **knowledge** of a model is stored in its weights, and the process of learning consists of finding “good values” for these weights—values that minimize a **loss** function. Because the chain of geometric transformations considered is differentiable, updating the weights to minimize the loss function is done efficiently via **gradient descent**.\n",
        "\n",
        "Even though deep learning is just one among many approaches to machine learning, it isn't on an equal footing with the others. Deep learning is a breakout success. Here's why."
      ],
      "metadata": {
        "id": "PXU1NVdzib0y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7gnQ7ZGYMxX"
      },
      "source": [
        "### What makes deep learning special within the field of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the span of only a few years, deep learning has achieved tremendous breakthroughs across a wide range of tasks that have been historically perceived as extremely difficult for computers, especially in the area of machine perception: extracting useful information from images, videos, sound, and more. Given sufficient training data (in particular, training data appropriately labeled by humans), deep learning makes it\n",
        "possible to extract from perceptual data almost anything a human could. Hence, it's sometimes said that deep learning has “solved perception”—although that's true only for a fairly narrow definition of perception.\n",
        "\n",
        "Due to its unprecedented technical successes, deep learning has singlehandedly brought about the third and by far the largest **AI summer**: a period of intense interest, investment, and hype in the field of AI. As this book is being written, we're in the middle of it. Whether this period will end in the near future, and what happens after it ends, are topics of debate. One thing is certain: in stark contrast with previous AI summers, deep learning has provided enormous business value to both large and small technology companies, enabling human-level speech recognition, smart assistants, human-level image classification, vastly improved machine translation, and more. The hype may (and likely will) recede, but the sustained economic and technological\n",
        "impact of deep learning will remain. In that sense, deep learning could be analogous to the internet: it may be overly hyped up for a few years, but in the longer term it will still be a major revolution that will transform our economy and our lives.\n",
        "\n",
        "I'm particularly optimistic about deep learning, because even if we were to make no further technological progress in the next decade, deploying existing algorithms to every applicable problem would be a game changer for most industries. Deep learning is nothing short of a revolution, and progress is currently happening at an incredibly fast rate, due to an exponential investment in resources and headcount. From where I stand, the future looks bright, although short-term expectations are\n",
        "somewhat overoptimistic; deploying deep learning to the full extent of its potential will likely take multiple decades."
      ],
      "metadata": {
        "id": "tVVDpKltjLd2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyRFIZc0YMxX"
      },
      "source": [
        "### How to think about deep learning\n",
        "\n",
        "The most surprising thing about deep learning is how simple it is. Ten years ago, no one expected that we would achieve such amazing results on machine-perception problems by using simple parametric models trained with gradient descent. Now, it turns out that all you need is sufficiently large parametric models trained with gradient descent on sufficiently many examples. As Feynman once said about the universe, “It's not complicated, it's just a lot of it.”\n",
        "\n",
        "In deep learning, everything is a vector—that is to say, everything is a **point** in a **geometric space**. Model inputs (text, images, and so on) and targets are first **vectorized**—\n",
        "turned into an initial input vector space and target vector space. Each layer in a deep learning model operates one simple geometric transformation on the data that goes through it. Together, the chain of layers in the model forms one complex geometric transformation, broken down into a series of simple ones. This complex transformation attempts to map the input space to the target space, one point at a time. This transformation is parameterized by the weights of the layers, which are iteratively\n",
        "updated based on how well the model is currently performing. A key characteristic of this geometric transformation is that it must be **differentiable**, which is required in order for us to be able to learn its parameters via gradient descent. Intuitively, this means the geometric morphing from inputs to outputs must be smooth and continuous—a significant constraint.\n",
        "\n",
        "The entire process of applying this complex geometric transformation to the input data can be visualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball is the manifold of the input data that the model\n",
        "starts with. Each movement operated by the person on the paper ball is similar to a simple geometric transformation operated by one layer. The full uncrumpling gesture sequence is the complex transformation of the entire model. Deep learning models are mathematical machines for uncrumpling complicated manifolds of high-dimensional data.\n",
        "\n",
        "That's the magic of deep learning: turning meaning into vectors, then into geometric spaces, and then incrementally learning complex geometric transformations that map one space to another. All you need are spaces of sufficiently high dimensionality in order to capture the full scope of the relationships found in the original data.\n",
        "\n",
        "The whole process hinges on a single core idea: that meaning is **derived from the pairwise relationship between things** (between words in a language, between pixels in an image, and so on) and that these **relationships can be captured by a distance function**. But note that whether the brain also implements meaning via geometric spaces is an entirely separate question. Vector spaces are efficient to work with from a computational standpoint, but different data structures for intelligence can easily be envisioned—in particular, graphs. Neural networks initially emerged from the idea of using graphs as a way to encode\n",
        "meaning, which is why they're named **neural networks**; the surrounding field of research used to be called **connectionism**. Nowadays the name “neural network” exists purely for historical reasons—it's an extremely misleading name because they're neither neural networks. In particular, neural networks have hardly anything to do with the brain. A more appropriate name would have been **layered representations learning or hierarchical representations learning**, or maybe even deep **differentiable models** or **chained geometric transforms**, to emphasize the fact that continuous geometric space manipulation is at their core."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub47w1AWYMxY"
      },
      "source": [
        "### Key enabling technologies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technological revolution that's currently unfolding didn't start with any single breakthrough invention. Rather, like any other revolution, it's the product of a vast accumulation of enabling factors—gradual at first, and then sudden. In the case of\n",
        "deep learning, we can point out the following key factors:\n",
        "\n",
        "- **Incremental algorithmic innovations**—These first began appearing slowly over the span of two decades (starting with backpropagation), and then were developed increasingly faster as more research effort was poured into deep learning after 2012.\n",
        "- **The availability of large amounts of perceptual data**—This was a requirement in order to realize that sufficiently large models trained on sufficiently large data are all we need. This is, in turn, a byproduct of the rise of the consumer internet and Moore's law applied to storage media.\n",
        "- **The availability of fast, highly parallel computation hardware at a low price**—Especially the GPUs produced by NVIDIA—first gaming GPUs and then chips designed from the ground up for deep learning. Early on, NVIDIA CEO Jensen Huang took note of the deep learning boom and decided to bet the company's future\n",
        "on it, which paid off in a big way.\n",
        "- **A complex stack of software layers that makes this computational power available to humans** —The CUDA language, frameworks like TensorFlow that do automatic differentiation, and Keras, which makes deep learning accessible to most people.\n",
        "\n",
        "In the future, deep learning will not be used only by specialists—researchers, graduate students, and engineers with an academic profile—it will be a tool in the toolbox of every developer, much like web technology today. Everyone needs to build intelligent apps: just as every business today needs a website, every product will need to intelligently make sense of user-generated data. Bringing about this future will require us to\n",
        "build tools that make deep learning radically easy to use and accessible to anyone with basic coding abilities. Keras has been the first major step in that direction."
      ],
      "metadata": {
        "id": "Sdtv6dyBmGRJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwqrcEo2YMxY"
      },
      "source": [
        "### The universal machine-learning workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having access to an extremely powerful tool for creating models that map any input space to any target space is great, but the difficult part of the machine learning workflow is often everything that comes before designing and training such models (and, for production models, what comes after, as well). Understanding the problem domain so as to be able to determine what to attempt to predict, given what data, and how to measure success, is a prerequisite for any successful application of machine learning, and it isn't something that advanced tools like Keras and TensorFlow can help you with. As a reminder, here's a quick summary of the typical machine learning workflow as described in chapter 6:\n",
        "\n",
        "1. Define the problem: What data is available, and what are you trying to predict? Will you need to collect more data or hire people to manually label a dataset?\n",
        "2. Identify a way to reliably measure success on your goal. For simple tasks this may be prediction accuracy, but in many cases it will require sophisticated, domain-specific metrics.\n",
        "3. Prepare the validation process that you'll use to evaluate your models. In particular, you should define a training set, a validation set, and a test set. The validation- and test-set labels shouldn't leak into the training data: for instance, with\n",
        "temporal prediction, the validation and test data should be posterior to the training data.\n",
        "4. Vectorize the data by turning it into vectors and preprocessing it in a way that makes it more easily approachable by a neural network (normalization and so on).\n",
        "5. Develop a first model that beats a trivial common-sense baseline, thus demonstrating that machine learning can work on your problem. This may not always be the case!\n",
        "6. Gradually refine your model architecture by tuning hyperparameters and adding regularization. Make changes based on performance on the validation data only, not the test data or the training data. Remember that you should get your model to overfit (thus identifying a model capacity level that's greater than you\n",
        "need) and only then begin to add regularization or downsize your model. Beware of validation-set overfitting when tuning hyperparameters—the fact that your hyperparameters may end up being overspecialized to the validation set. Avoiding this is the purpose of having a separate test set.\n",
        "7. Deploy your final model in production—as a web API, as part of a JavaScript or C++ application, on an embedded device, etc. Keep monitoring its performance on real-world data, and use your findings to refine the next iteration of the model!"
      ],
      "metadata": {
        "id": "dB5xawwhm_Gv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NQ_u6-OYMxY"
      },
      "source": [
        "### Key network architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The four families of network architectures that you should be familiar with are **densely connected networks, convolutional networks, recurrent networks, and Transformers**. Each type\n",
        "of model is meant for a specific input modality. A network architecture encodes **assumptions** about the structure of the data: a **hypothesis space** within which the search for a good model will proceed. Whether a given architecture will work on a given problem depends entirely on the match between the structure of the data and the assumptions of the network architecture.\n",
        "\n",
        "These different network types can easily be combined to achieve larger multimodal models, much as you combine LEGO bricks. In a way, deep learning layers are LEGO bricks for information processing. Here's a quick overview of the mapping between input modalities and appropriate network architectures:"
      ],
      "metadata": {
        "id": "DlfcdMCQn3Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Vector data**—Densely connected models (Dense layers).\n",
        "- **Image data**—2D convnets.\n",
        "- **Sequence data**—RNNs for timeseries, or Transformers for discrete sequences (such as sequences of words). 1D convnets can also be used for translation-invariant, continuous sequence data, such as birdsong waveforms.\n",
        "- **Video data**—Either 3D convnets (if you need to capture motion effects), or a combination of a frame-level 2D convnet for feature extraction followed by a sequence-processing model.\n",
        "- **Volumetric data**—3D convnets.\n",
        "\n",
        "Now, let's quickly review the specificities of each network architecture."
      ],
      "metadata": {
        "id": "cw1O_AGxoMw8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uMWppQqYMxZ"
      },
      "source": [
        "#### Densely connected networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A densely connected network is a stack of **Dense** layers meant to process vector data (where each sample is a vector of numerical or categorical attributes). Such networks assume no specific structure in the input features: they're called **densely connected** because the units of a Dense layer are connected to every other unit. The layer attempts to map relationships between any two input features; this is unlike a 2D convolution layer, for instance, which only looks at **local** relationships.\n",
        "\n",
        "Densely connected networks are most commonly used for **categorical data** (for example, where the input features are lists of attributes), such as the Boston Housing Price dataset used in chapter 4. They're also used as the final classification or regression stage of most networks. For instance, the convnets covered in chapter 8 typically end with one or two Dense layers, and so do the recurrent networks in chapter 10.\n",
        "\n",
        "Remember, to perform **binary classification**, end your stack of layers with a `Dense` layer with a single unit and a **sigmoid** activation, and use `binary_crossentropy` as the loss. Your targets should be either 0 or 1:"
      ],
      "metadata": {
        "id": "rZlmK8MRonKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hj22H6sjYMxZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "collapsed": true,
        "outputId": "bb6442c1-f897-4b8b-e79c-1b4b2cd97dd3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_input_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4cf61e86f2f0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_input_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_input_features' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform **single-label categorical classification** (where each sample has exactly one class, no more), end your stack of layers with a `Dense` layer with a number of units equal to the\n",
        "number of classes, and a softmax activation. If your targets are one-hot encoded, use `categorical_crossentropy` as the loss; if they're integers, use `sparse_categorical_crossentropy`:"
      ],
      "metadata": {
        "id": "lXdiv0zSpbwn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nonBJcBhYMxa"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform **multilabel categorical classification** (where each sample can have several classes), end your stack of layers with a `Dense` layer with a number of units equal to the number of classes, and a **sigmoid activation**, and use `binary_crossentropy` as the loss. Your targets should be multi-hot encoded:"
      ],
      "metadata": {
        "id": "Akp_HdO9psqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-bnvCvpYMxb"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32,activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32,activation=\"relu\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform **regression** toward a vector of continuous values, end your stack of layers with a Dense layer with a number of units equal to the number of values you're trying to predict (often a single one, such as the price of a house), and no activation. Various losses can be used for regression—most commonly `mean_squared_error` (MSE):"
      ],
      "metadata": {
        "id": "kE12gmt1p5DF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPrhIgJJYMxb"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs layers.Dense(num_values)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ljuW-xVYMxc"
      },
      "source": [
        "#### Convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution layers look at spatially local patterns by applying the same geometric transformation to different spatial locations (**patches**) in an input tensor. This results in representations that are **translation invariant**, making convolution layers highly data efficient and modular. This idea is applicable to spaces of any dimensionality: 1D (continuous sequences), 2D (images), 3D (volumes), and so on. You can use the `Conv1D` layer to process sequences, the `Conv2D` layer to process images, and the `Conv3D` layers to process volumes. As a leaner, more efficient alternative to convolution layers, you can also use **depthwise separable convolution layers**, such as `SeparableConv2D`.\n",
        "\n",
        "**Convnets**, or **convolutional networks**, consist of stacks of convolution and max-pooling layers. The pooling layers let you spatially downsample the data, which is required to keep feature maps to a reasonable size as the number of features grows, and to allow subsequent convolution layers to “see” a greater spatial extent of the inputs. Convnets are often ended with either a `Flatten` operation or a global pooling layer, turning spatial feature maps into vectors, followed by `Dense` layers to achieve classification or regression.\n",
        "\n",
        "Here's a typical image-classification network (categorical classification, in this case), leveraging `SeparableConv2D` layers:"
      ],
      "metadata": {
        "id": "p9VNdC8TqjW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hzilRLkSYMxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "600bc768-681a-47bc-bca2-ace4ae6ef02a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid non-printable character U+00A0 (<ipython-input-1-09391fa45564>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-09391fa45564>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    inputs = keras.Input(shape=(height, width, channels))\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(height, width, channels))\n",
        "x = layers.SeparableConv2D(32,3,activation=\"relu\")(inputs)\n",
        "x = layers.SeparableConv2D(64,3,activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.SeparableConv2D(64,3,activation=\"relu\")(x)\n",
        "x = layers.SeparableConv2D(128,3,activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.SeparableConv2D(64,3,activation=\"relu\")(x)\n",
        "x = layers.SeparableConv2D(128,3,activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(32,activation=\"relu\")(x)\n",
        "outputs = layers.Dense(num_classes,activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building a very deep convnet, it's common to add **batch normalization** layers as well as **residual connections**—two architecture patterns that help gradient information flow smoothly through the network."
      ],
      "metadata": {
        "id": "oR5DgTwprJoH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2YW64bFYMxc"
      },
      "source": [
        "#### RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent neural networks** (RNNs) work by processing sequences of inputs one timestep at a time, and maintaining a state throughout (a state is typically a vector or set of vectors). They should be used preferentially over 1D convnets in the case of sequences where patterns of interest aren't invariant by temporal translation (for instance, timeseries data where the recent past is more important than the distant past).\n",
        "\n",
        "Three RNN layers are available in Keras: `SimpleRNN`, `GRU`, and `LSTM`. For most practical purposes, **you should use either GRU or LSTM**. LSTM is the more powerful of the two but is also more expensive; you can think of GRU as a simpler, cheaper alternative to it.\n",
        "\n",
        "In order to stack multiple RNN layers on top of each other, each layer prior to the last layer in the stack should return the full sequence of its outputs (each input timestep will correspond to an output timestep). If you aren't stacking any further RNN\n",
        "layers, it's common to return only the last output, which contains information about the entire sequence.\n",
        "\n",
        "Following is a single RNN layer for binary classification of vector sequences:"
      ],
      "metadata": {
        "id": "EfQ27Or2rWcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atTIzxRpYMxc"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_timesteps,num_features))\n",
        "x = layers.LSTM(32)(inputs)\n",
        "outputs = layers.Dense(num_classes,activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this is a stacked RNN for binary classification of vector sequences:"
      ],
      "metadata": {
        "id": "KQc0eARXsDW6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHIFfoarYMxd"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_timesteps,num_features))\n",
        "x = layers.LSTM(32,return_sequences=True)(inputs)\n",
        "x = layers.LSTM(32,return_sequences=True)(x)\n",
        "x = layers.LSTM(32)(x)\n",
        "outputs = layers.Dense(num_classes,activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6-qqeI-YMxd"
      },
      "source": [
        "#### Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Transformer looks at a set of vectors (such as word vectors), and leverages **neural attention** to transform each vector into a representation that is aware of the **context** provided by the other vectors in the set. When the set in question is an ordered sequence, you can also leverage **positional encoding** to create Transformers that can take intoaccount both global context and word order, capable of processing long text paragraphs much more effectively than RNNs or 1D convnets.\n",
        "\n",
        "Transformers can be used for any set-processing or sequence-processing task, including text classification, but they excel especially at **sequence-to-sequence learning**, such as translating paragraphs in a source language into a target language.\n",
        "\n",
        "A sequence-to-sequence Transformer is made up of two parts:\n"
      ],
      "metadata": {
        "id": "J-rQ6cmvsIWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A **TransformerEncoder** that turns an input vector sequence into a context-aware, order-aware output vector sequence\n",
        "2. A **TransformerDecoder** that takes the output of the **TransformerEncoder**, as well as a target sequence, and predicts what should come next in the target sequence\n",
        "\n",
        "If you're only processing a single sequence (or set) of vectors, you'd be only using the TransformerEncoder.\n",
        "\n",
        "Following is a sequence-to-sequence Transformer for mapping a source sequence to a target sequence (this setup could be used for machine translation or question answering, for instance):"
      ],
      "metadata": {
        "id": "PdAu7KO5sg2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO2a0GRUYMxd"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")           # Source sequence\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\")                      # Target sequence so far\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)             # Target sequence one step in the future\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "transformer.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this is a lone TransformerEncoder for binary classification of integer sequences:"
      ],
      "metadata": {
        "id": "KT9ifTFetDFO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsRIbUvYYMxd"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full implementations of the TransformerEncoder, the TransformerDecoder, and the PositionalEmbedding layer are provided in chapter 11."
      ],
      "metadata": {
        "id": "UjB-MDzVtHwI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_PW3aTYMxd"
      },
      "source": [
        "### The space of possibilities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What will you build with these techniques? Remember, building deep learning models is like playing with LEGO bricks: layers can be plugged together to map essentially anything to anything, given that you have appropriate training data available and that\n",
        "the mapping is achievable via a continuous geometric transformation of reasonable complexity. The space of possibilities is infinite. This section offers a few examples to\n",
        "inspire you to think beyond the basic classification and regression tasks that have traditionally been the bread and butter of machine learning.\n",
        "\n",
        "I've sorted my suggested applications by input and output modalities in the following list. Note that quite a few of them stretch the limits of what is possible—although a model could be trained on all of these tasks, in some cases such a model probably\n",
        "wouldn't generalize far from its training data. The later sections will address how these limitations could be lifted in the future:"
      ],
      "metadata": {
        "id": "FfxOQIkEtO6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mapping vector data to vector data:\n",
        "  - Predictive healthcare—Mapping patient medical records to predictions of\n",
        "patient outcomes\n",
        "  - Behavioral targeting—Mapping a set of website attributes with data on how\n",
        "long a user will spend on the website\n",
        "  - Product quality control—Mapping a set of attributes relative to an instance of a manufactured product with the probability that the product will fail by next year\n",
        "\n",
        "* Mapping image data to vector data:\n",
        "  - Medical assistant—Mapping slides of medical images to a prediction about the presence of a tumor\n",
        "  - Self-driving vehicle—Mapping car dashcam video frames to steering wheel angle commands and gas and braking commands\n",
        "  - Board game AI—Mapping Go or chess boards to the next player move\n",
        "  - Diet helper—Mapping pictures of a dish to its calorie count\n",
        "  - Age prediction—Mapping selfies to the age of the person\n",
        "\n",
        "* Mapping timeseries data to vector data:\n",
        "  - Weather prediction—Mapping timeseries of weather data in a grid of locations to the temperature in a specific place one week later\n",
        "  - Brain-computer interfaces—Mapping timeseries of magnetoencephalogram (MEG) data to computer commands\n",
        "  - Behavioral targeting—Mapping timeseries of user interactions on a website to the probability that a user will buy something\n",
        "\n",
        "* Mapping text to text:\n",
        "  - Machine translation— Mapping a paragraph in one language to a translated version in a different language\n",
        "  - Smart reply—Mapping emails to possible one-line replies\n",
        "  - Question answering—Mapping general-knowledge questions to answers\n",
        "  - Summarization—Mapping a long article to a short summary of the article\n",
        "\n",
        "* Mapping images to text:\n",
        "  - Text transcription— Mapping images that contain a text element to the corresponding text string\n",
        "  - Captioning—Mapping images to short captions describing the contents of the images\n",
        "\n",
        "* Mapping text to images:\n",
        "  - Conditioned image generation—Mapping a short text description to images matching the description\n",
        "  - Logo generation/selection—Mapping the name and description of a company to a logo suggestion\n",
        "\n",
        "* Mapping images to images:\n",
        "  - Super-resolution—Mapping downsized images to higher-resolution versions of the same images\n",
        "  - Visual depth sensing—Mapping images of indoor environments to maps of depth predictions\n",
        "\n",
        "* Mapping images and text to text:\n",
        "  - Visual QA—Mapping images and natural language questions about the contents of images to natural language answers\n",
        "\n",
        "* Mapping video and text to text:\n",
        "  - Video QA—Mapping short videos and natural language questions about the contents of videos to natural language answers\n",
        "\n",
        "**Almost anything** is possible, but not quite **anything**. You'll see in the next section what we **can't** do with deep learning."
      ],
      "metadata": {
        "id": "9LU8FXTTthM5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qayVCxBYMxe"
      },
      "source": [
        "## The limitations of deep learning\n",
        "\n",
        "The space of applications that can be implemented with deep learning is infinite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnYFdnUbYMxe"
      },
      "source": [
        "### The risk of anthropomorphizing machine-learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9lFWUElYMxe"
      },
      "source": [
        "### Automatons vs. intelligent agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q_D1hL0YMxe"
      },
      "source": [
        "### Local generalization vs. extreme generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHpxDxZFYMxe"
      },
      "source": [
        "### The purpose of intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANNf9GC9YMxf"
      },
      "source": [
        "### Climbing the spectrum of generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xr3IvXWYMxf"
      },
      "source": [
        "## Setting the course toward greater generality in AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SXWJbNXYMxf"
      },
      "source": [
        "### On the importance of setting the right objective: The shortcut rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbH6YZ59YMxf"
      },
      "source": [
        "### A new target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRWy2TMvYMxf"
      },
      "source": [
        "## Implementing intelligence: The missing ingredients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nch2SKRtYMxf"
      },
      "source": [
        "### Intelligence as sensitivity to abstract analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMaaYr6BYMxf"
      },
      "source": [
        "### The two poles of abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVTjwPLnYMxf"
      },
      "source": [
        "#### Value-centric analogy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVO9eGWYMxf"
      },
      "source": [
        "#### Program-centric analogy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WTNM_ZoYMxg"
      },
      "source": [
        "#### Cognition as a combination of both kinds of abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EU6R0ZZYMxg"
      },
      "source": [
        "### The missing half of the picture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuD29Wq7YMxg"
      },
      "source": [
        "## The future of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXJoDfsuYMxg"
      },
      "source": [
        "### Models as programs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tedihIjRYMxg"
      },
      "source": [
        "### Blending together deep learning and program synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpw8pu-GYMxg"
      },
      "source": [
        "#### Integrating deep-learning modules and algorithmic modules into hybrid systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U52CTmWPYMxh"
      },
      "source": [
        "#### Using deep learning to guide program search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPW1cjWIYMxh"
      },
      "source": [
        "### Lifelong learning and modular subroutine reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txS0bC5vYMxh"
      },
      "source": [
        "### The long-term vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhJ2K82kYMxh"
      },
      "source": [
        "## Staying up to date in a fast-moving field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FcrRHH5YMxh"
      },
      "source": [
        "### Practice on real-world problems using Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr59DgjyYMxh"
      },
      "source": [
        "### Read about the latest developments on arXiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ASBIxJ9YMxi"
      },
      "source": [
        "### Explore the Keras ecosystem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DVEsWT2YMxi"
      },
      "source": [
        "## Final words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}