{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55048dc7ff5d406eb236dab70e6a064e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_c6d7f8a8801a4190869ee753a617af60"
          }
        },
        "18a421659eab4059a73f1fd35910adc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9b71c672ff49b3802d43e3f915f3ae",
            "placeholder": "​",
            "style": "IPY_MODEL_cbf7149bdcae474689dd0e906014695f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "fb11b2c136b34891ac5efbd983723049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6f5a6521a53f4018b789c6485a2a0a44",
            "placeholder": "​",
            "style": "IPY_MODEL_8b7ffc03093e4a9d82fe8b85bc800976",
            "value": ""
          }
        },
        "5bcd64d3959541e0b7d8852581487cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_8371d318ee324178a1bec58ad629b28f",
            "style": "IPY_MODEL_93c230632d67438f9fa8c6921c6f3b25",
            "value": true
          }
        },
        "32cb4f391d3e409c9801f208e797b138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_89e5e9955b24451f813cffc5bbc6cf72",
            "style": "IPY_MODEL_7dc2a017f328439382b0093d8fef023b",
            "tooltip": ""
          }
        },
        "c00a291532f544b3b9047b70ae9007a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63818e4a2f56421385c48eb13807dd20",
            "placeholder": "​",
            "style": "IPY_MODEL_0cc6bb7673c84ffdb20435c4b2d5197b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "c6d7f8a8801a4190869ee753a617af60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ec9b71c672ff49b3802d43e3f915f3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf7149bdcae474689dd0e906014695f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f5a6521a53f4018b789c6485a2a0a44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7ffc03093e4a9d82fe8b85bc800976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8371d318ee324178a1bec58ad629b28f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c230632d67438f9fa8c6921c6f3b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89e5e9955b24451f813cffc5bbc6cf72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc2a017f328439382b0093d8fef023b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "63818e4a2f56421385c48eb13807dd20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc6bb7673c84ffdb20435c4b2d5197b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ec5003a8de24727bf301af52f965cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38ad554a16bb431c899a68ff0f34233b",
            "placeholder": "​",
            "style": "IPY_MODEL_ae58fdf9a2c34c2f97f400c5ddb742f3",
            "value": "Connecting..."
          }
        },
        "38ad554a16bb431c899a68ff0f34233b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae58fdf9a2c34c2f97f400c5ddb742f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
	"state": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fast tokenizers' special powers\n"
      ],
      "metadata": {
        "id": "0uJ341A3lbyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ],
      "metadata": {
        "id": "04QQ-_WBI0wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "SeZphYRII3kt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, log into Hugging face."
      ],
      "metadata": {
        "id": "rtr61gv6I5Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "55048dc7ff5d406eb236dab70e6a064e",
            "18a421659eab4059a73f1fd35910adc3",
            "fb11b2c136b34891ac5efbd983723049",
            "5bcd64d3959541e0b7d8852581487cca",
            "32cb4f391d3e409c9801f208e797b138",
            "c00a291532f544b3b9047b70ae9007a7",
            "c6d7f8a8801a4190869ee753a617af60",
            "ec9b71c672ff49b3802d43e3f915f3ae",
            "cbf7149bdcae474689dd0e906014695f",
            "6f5a6521a53f4018b789c6485a2a0a44",
            "8b7ffc03093e4a9d82fe8b85bc800976",
            "8371d318ee324178a1bec58ad629b28f",
            "93c230632d67438f9fa8c6921c6f3b25",
            "89e5e9955b24451f813cffc5bbc6cf72",
            "7dc2a017f328439382b0093d8fef023b",
            "63818e4a2f56421385c48eb13807dd20",
            "0cc6bb7673c84ffdb20435c4b2d5197b",
            "9ec5003a8de24727bf301af52f965cd2",
            "38ad554a16bb431c899a68ff0f34233b",
            "ae58fdf9a2c34c2f97f400c5ddb742f3"
          ]
        },
        "id": "WFGvurNJI6tN",
        "outputId": "4caee845-a473-4041-8ba2-2788a06fcd1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55048dc7ff5d406eb236dab70e6a064e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will take a closer look at the <font color='blue'>capabilities</font> of the <font color='blue'>tokenizers</font> in 🤗 Transformers. Up to now we have only used them to <font color='blue'>tokenize inputs</font> or <font color='blue'>decode IDs</font> back into text, but tokenizers -- especially those backed by the 🤗 Tokenizers library -- can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](https://huggingface.co/learn/llm-course/chapter1/3).\n",
        "\n",
        "In the following discussion, we will often make the distinction between <font color='blue'>slow</font> and <font color='blue'>fast</font> tokenizers. <font color='blue'>Slow</font> tokenizers are those <font color='blue'>written in Python</font> inside the 🤗 Transformers library, while the <font color='blue'>fast</font> versions are the ones provided by 🤗 Tokenizers, which are <font color='blue'>written in Rust</font>. If you remember the table from [Chapter 5](https://huggingface.co/learn/llm-course/chapter5/3) that reported how long it took a fast and a slow tokenizer to tokenize the Drug Review Dataset, you should have an idea of why we call them fast and slow:\n",
        "\n",
        "|               | Fast tokenizer | Slow tokenizer\n",
        ":--------------:|:--------------:|:-------------:\n",
        "`batched=True`  | 10.8s          | 4min41s\n",
        "`batched=False` | 59.2s          | 5min3s\n",
        "\n",
        "<Tip warning={true}>\n",
        "\n",
        "⚠️ When <font color='blue'>tokenizing</font> a <font color='blue'>single sentence</font>, you won't always <font color='blue'>see a difference</font> in speed between the slow and fast versions of the same tokenizer. In fact, the <font color='blue'>fast version</font> might actually be <font color='blue'>slower</font>! It's only when <font color='blue'>tokenizing</font> lots of <font color='blue'>texts</font> in <font color='blue'>parallel</font> at the same time that you will be able to clearly see the difference.\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "bOUjnXBFlxCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch encoding\n",
        "\n",
        "\n",
        "The <font color='blue'>output</font> of a <font color='blue'>tokenizer</font> isn't a simple Python dictionary; what we get is actually a special <font color='blue'>`BatchEncoding` object</font>. It's a <font color='blue'>subclass</font> of a <font color='blue'>dictionary</font> (which is why we were able to index into that result without any problem before), but with <font color='blue'>additional methods</font> that are mostly used by <font color='blue'>fast tokenizers</font>.\n",
        "\n",
        "Besides their parallelization capabilities, the <font color='blue'>key functionality</font> of <font color='blue'>fast tokenizers</font> is that they always keep track of the <font color='blue'>original span</font> of <font color='blue'>texts</font> the <font color='blue'>final tokens come from</font> -- a feature we call <font color='blue'>offset mapping</font>. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it's inside, and vice versa.\n",
        "\n",
        "Let's take a look at an example:"
      ],
      "metadata": {
        "id": "8oNFvvL4l1nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "encoding = tokenizer(example)\n",
        "print(type(encoding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NauWcRcZl2hk",
        "outputId": "2ec36ca3-5617-46c8-8c6d-7c8507882c65"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned previously, we get a <font color='blue'>BatchEncoding object</font> in the <font color='blue'>tokenizer's output</font>. Since the `AutoTokenizer` class picks a <font color='blue'>fast tokenizer</font> by <font color='blue'>default</font>, we can use the <font color='blue'>additional methods</font> this <font color='blue'>`BatchEncoding`</font> object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute `is_fast` of the `tokenizer`:\n"
      ],
      "metadata": {
        "id": "kz9MQTiWl-6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA5VTFNYmBA7",
        "outputId": "92243dc1-baf4-48b0-df80-1cba1af7272a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "or check the same attribute of our `encoding`:"
      ],
      "metadata": {
        "id": "MW1VQi2RmG8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.is_fast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do3ulWhtmIe3",
        "outputId": "63debba1-8bb7-42b1-d768-01a81f1f01f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what a <font color='blue'>fast tokenizer</font> enables us to do. First, we can <font color='blue'>access the tokens</font> without having to convert the IDs back to tokens:"
      ],
      "metadata": {
        "id": "98b-oDw6mK-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.tokens()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP2T0I_ImM0g",
        "outputId": "003ff402-d67e-49c9-a4f8-1206ebae0fb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'S',\n",
              " '##yl',\n",
              " '##va',\n",
              " '##in',\n",
              " 'and',\n",
              " 'I',\n",
              " 'work',\n",
              " 'at',\n",
              " 'Hu',\n",
              " '##gging',\n",
              " 'Face',\n",
              " 'in',\n",
              " 'Brooklyn',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case the token at <font color='blue'>index 5</font> is <font color='blue'>`##yl`</font>, which is part of the word <font color='blue'>Sylvain</font> in the original sentence. We can also use the <font color='blue'>`word_ids()` method</font> to get the <font color='blue'>index of the word</font> each token comes from:\n"
      ],
      "metadata": {
        "id": "vCcXB_eXmOpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.word_ids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuz11LUKmQ2t",
        "outputId": "fecef232-687b-4c44-a20a-00a312ce023e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the tokenizer's special tokens <font color='blue'>`[CLS]`</font> and <font color='blue'>`[SEP]`</font> are mapped to <font color='blue'>`None`</font>, and then each <font color='blue'>token</font> is <font color='blue'>mapped</font> to the <font color='blue'>word</font> it <font color='blue'>originates from</font>. This is especially useful to determine if a <font color='blue'>token</font> is at the <font color='blue'>start</font> of a <font color='blue'>word</font> or if <font color='blue'>two tokens</font> are <font color='blue'>in</font> the <font color='blue'>same word</font>. We could rely on the <font color='blue'>`##` prefix</font> for that, but it only works for <font color='blue'>BERT-like tokenizers</font>; this method works for any type of tokenizer as long as it's a fast one. In the next chapter, we'll see how we can use this capability to apply the <font color='blue'>labels</font> we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to <font color='blue'>mask all the tokens</font> coming from the <font color='blue'>same word</font> in masked language modeling (a technique called <font color='blue'>whole word masking</font>).\n",
        "\n",
        "<Tip>\n",
        "\n",
        "The notion of what a word is complicated. For instance, does \"I'll\" (a contraction of \"I will\") count as one or two words? It actually <font color='blue'>depends</font> on the <font color='blue'>tokenizer</font> and the <font color='blue'>pre-tokenization operation</font> it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words.\n",
        "\n",
        "✏️ **Try it out!** Create a tokenizer from the `bert-base-cased` and `roberta-base` checkpoints and <font color='blue'>tokenize \"81s\"</font> with them. What do you observe? What are the word IDs?\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "NuSpMChRmTPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Create tokenizers for BERT and RoBERTa\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "test_string = \"81s\"\n",
        "\n",
        "# Tokenization with BERT\n",
        "bert_encoding = bert_tokenizer(test_string)\n",
        "print(\"Tokenization with BERT\\n\")\n",
        "print(f\"Input: '{test_string}'\")\n",
        "print(f\"Tokens: {bert_tokenizer.tokenize(test_string)}\")\n",
        "print(f\"Token IDs: {bert_encoding['input_ids']}\")\n",
        "print(f\"Word IDs: {bert_encoding.word_ids()}\\n\")\n",
        "\n",
        "# Tokenization with RoBERTa\n",
        "roberta_encoding = roberta_tokenizer(test_string)\n",
        "print(\"Tokenization with RoBERTa\\n\")\n",
        "print(f\"Input: '{test_string}'\")\n",
        "print(f\"Tokens: {roberta_tokenizer.tokenize(test_string)}\")\n",
        "print(f\"Token IDs: {roberta_encoding['input_ids']}\")\n",
        "print(f\"Word IDs: {roberta_encoding.word_ids()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Byxxjv1u9L",
        "outputId": "4e44b3eb-ee81-4a4c-e550-3bdc5ca7ae43"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization with BERT\n",
            "\n",
            "Input: '81s'\n",
            "Tokens: ['81', '##s']\n",
            "Token IDs: [101, 5615, 1116, 102]\n",
            "Word IDs: [None, 0, 0, None]\n",
            "\n",
            "Tokenization with RoBERTa\n",
            "\n",
            "Input: '81s'\n",
            "Tokens: ['81', 's']\n",
            "Token IDs: [0, 6668, 29, 2]\n",
            "Word IDs: [None, 0, 1, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This comparison shows <font color='blue'>differences</font> between the <font color='blue'>BERT</font> and <font color='blue'>RoBERTa tokenization</font> strategies. <font color='blue'>BERT</font> employs the <font color='blue'>WordPiece algorithm</font>, which splits `81s` into subword pieces (`81` + `##s`),  with <font color='blue'>both tokens</font> receiving <font color='blue'>`word_id = 0`</font> since they <font color='blue'>originate</font> from the <font color='blue'>same word</font>. In contrast, <font color='blue'>RoBERTa</font> tokenizes `81s` as `['81', 's']` with word IDs `[None, 0, 1, None]`, meaning it treats <font color='blue'>`81`</font> and <font color='blue'>`s`</font> as <font color='blue'>separate words</font> rather than <font color='blue'>parts</font> of the <font color='blue'>same word</font> like BERT does. Both approaches assign `word_id = None` to their respective special tokens (`[CLS]/[SEP]` for BERT, `<s>/</s>` for RoBERTa)."
      ],
      "metadata": {
        "id": "soA-VntZ2uRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, there is a <font color='blue'>`sentence_ids()` method</font> that we can use to <font color='blue'>map</font> a <font color='blue'>token</font> to the <font color='blue'>sentence it came from</font> (though in this case, the `token_type_ids` returned by the tokenizer can give us the same information).\n",
        "\n",
        "Lastly, we can map <font color='blue'>any word</font> or <font color='blue'>token</font> to <font color='blue'>characters</font> in the <font color='blue'>original text</font>, and vice versa, via the `word_to_chars()` or `token_to_chars()` and `char_to_word()` or `char_to_token()` methods. For instance, the <font color='blue'>`word_ids()` method</font> told us that <font color='blue'>`##yl`</font> is <font color='blue'>part</font> of the <font color='blue'>word</font> at <font color='blue'>index 3</font>, but which word is it in the sentence? We can find out like this:"
      ],
      "metadata": {
        "id": "g9PVZ0bJrrrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start, end = encoding.word_to_chars(3)\n",
        "example[start:end]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k6_B4DbnmW04",
        "outputId": "f108b8ca-9be6-46f9-8c31-dc255e0e5dfe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sylvain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned previously, this is all powered by the fact the <font color='blue'>fast tokenizer</font> keeps track of the <font color='blue'>span of text</font> each <font color='blue'>token comes from</font> in a list of <font color='blue'>offsets</font>. To illustrate their use, next we'll show you how to replicate the results of the `token-classification` pipeline manually.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "✏️ **Try it out!** Create your own example text and see if you can understand which tokens are associated with word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you.\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "sThSrz55mZjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Example sentence for tokens associated with a word ID\n",
        "print('Example of a single sentence\\n')\n",
        "example = \"OpenAI's ChatGPT revolutionized the ability of users to get questionable information.\"\n",
        "encoding = tokenizer(example)\n",
        "\n",
        "print(f\"Sentence: {example}\")\n",
        "print(f\"Tokens: {tokenizer.tokenize(example)}\")\n",
        "print(f\"Word IDs: {encoding.word_ids()}\")\n",
        "\n",
        "# Character span for word 3 (ChatGPT)\n",
        "word_id = 3\n",
        "start, end = encoding.word_to_chars(word_id)\n",
        "print(f\"Word {word_id}: '{example[start:end]}'\")\n",
        "\n",
        "# Example for two sentences\n",
        "print('\\nExample for two sentences\\n')\n",
        "sentence1 = \"I love analytic number theory!\"\n",
        "sentence2 = \"The Riemann zeta function is the most interesting thing on planet Earth.\"\n",
        "two_sent_encoding = tokenizer(sentence1, sentence2)\n",
        "\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(two_sent_encoding['input_ids'])}\")\n",
        "print(f\"Word IDs: {two_sent_encoding.word_ids()}\")\n",
        "print(f\"Token Type IDs: {two_sent_encoding['token_type_ids']}\")\n",
        "\n",
        "# Sentence mapping via token_type_ids (0 = first sentence, 1 = second sentence)\n",
        "print('\\nSentence mapping via token_type_ids\\n')\n",
        "tokens = tokenizer.convert_ids_to_tokens(two_sent_encoding['input_ids'])\n",
        "for i, (token, type_id) in enumerate(zip(tokens, two_sent_encoding['token_type_ids'])):\n",
        "    sentence = \"First sentence\" if type_id == 0 else \"Second sentence\"\n",
        "    print(f\"{token} -> {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eujGtLzU6pMf",
        "outputId": "305eb1a5-af5e-4733-8162-66c489188f43"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of a single sentence\n",
            "\n",
            "Sentence: OpenAI's ChatGPT revolutionized the ability of users to get questionable information.\n",
            "Tokens: ['Open', '##A', '##I', \"'\", 's', 'Cha', '##t', '##GP', '##T', 'revolution', '##ized', 'the', 'ability', 'of', 'users', 'to', 'get', 'questionable', 'information', '.']\n",
            "Word IDs: [None, 0, 0, 0, 1, 2, 3, 3, 3, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, None]\n",
            "Word 3: 'ChatGPT'\n",
            "\n",
            "Example for two sentences\n",
            "\n",
            "Sentence 1: I love analytic number theory!\n",
            "Sentence 2: The Riemann zeta function is the most interesting thing on planet Earth.\n",
            "Tokens: ['[CLS]', 'I', 'love', 'anal', '##ytic', 'number', 'theory', '!', '[SEP]', 'The', 'R', '##ie', '##mann', 'z', '##eta', 'function', 'is', 'the', 'most', 'interesting', 'thing', 'on', 'planet', 'Earth', '.', '[SEP]']\n",
            "Word IDs: [None, 0, 1, 2, 2, 3, 4, 5, None, 0, 1, 1, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
            "Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Sentence mapping via token_type_ids\n",
            "\n",
            "[CLS] -> First sentence\n",
            "I -> First sentence\n",
            "love -> First sentence\n",
            "anal -> First sentence\n",
            "##ytic -> First sentence\n",
            "number -> First sentence\n",
            "theory -> First sentence\n",
            "! -> First sentence\n",
            "[SEP] -> First sentence\n",
            "The -> Second sentence\n",
            "R -> Second sentence\n",
            "##ie -> Second sentence\n",
            "##mann -> Second sentence\n",
            "z -> Second sentence\n",
            "##eta -> Second sentence\n",
            "function -> Second sentence\n",
            "is -> Second sentence\n",
            "the -> Second sentence\n",
            "most -> Second sentence\n",
            "interesting -> Second sentence\n",
            "thing -> Second sentence\n",
            "on -> Second sentence\n",
            "planet -> Second sentence\n",
            "Earth -> Second sentence\n",
            ". -> Second sentence\n",
            "[SEP] -> Second sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inside the `token-classification` pipeline"
      ],
      "metadata": {
        "id": "QON0KlEOm44B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In [Chapter 1](https://huggingface.co/learn/llm-course/chapter1/3) we got our first taste of applying <font color='blue'>NER</font> -- where the task is to identify which <font color='blue'>parts of the text</font> correspond to <font color='blue'>entities</font> like <font color='blue'>persons, locations, or organizations</font> -- with the 🤗 Transformers `pipeline()` function. Then, in [Chapter 2](https://huggingface.co/learn/llm-course/chapter2/2), we saw how a <font color='blue'>pipeline groups together</font> the <font color='blue'>three stages</font> necessary to get the <font color='blue'>predictions</font> from a <font color='blue'>raw text</font>: tokenization, passing the inputs through the model, and post-processing. The <font color='blue'>first two steps</font> in the `token-classification` pipeline are the <font color='blue'>same</font> as in any other pipeline, but the <font color='blue'>post-processing</font> is a little <font color='blue'>more complex</font> -- let's see how!\n"
      ],
      "metadata": {
        "id": "Zv5zsLqbm9V8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the base results with the pipeline\n",
        "\n",
        "First, let's grab a token classification pipeline so we can get some results to compare manually. The model used by default is [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); it performs NER on sentences:\n"
      ],
      "metadata": {
        "id": "9QHG109fnHiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU4B1bIHnIuO",
        "outputId": "b50ccdee-5cf9-4139-fa2e-3e4b5b847a57"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER',\n",
              "  'score': np.float32(0.99938285),\n",
              "  'index': 4,\n",
              "  'word': 'S',\n",
              "  'start': 11,\n",
              "  'end': 12},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99815494),\n",
              "  'index': 5,\n",
              "  'word': '##yl',\n",
              "  'start': 12,\n",
              "  'end': 14},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99590707),\n",
              "  'index': 6,\n",
              "  'word': '##va',\n",
              "  'start': 14,\n",
              "  'end': 16},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99923277),\n",
              "  'index': 7,\n",
              "  'word': '##in',\n",
              "  'start': 16,\n",
              "  'end': 18},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.9738931),\n",
              "  'index': 12,\n",
              "  'word': 'Hu',\n",
              "  'start': 33,\n",
              "  'end': 35},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.976115),\n",
              "  'index': 13,\n",
              "  'word': '##gging',\n",
              "  'start': 35,\n",
              "  'end': 40},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.9887976),\n",
              "  'index': 14,\n",
              "  'word': 'Face',\n",
              "  'start': 41,\n",
              "  'end': 45},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.9932106),\n",
              "  'index': 16,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <font color='blue'>model</font> properly <font color='blue'>identified</font> each <font color='blue'>token</font> generated by <font color='blue'>Sylvain</font> as a <font color='blue'>person</font>, each token generated by <font color='blue'>Hugging Face</font> as an <font color='blue'>organization</font>, and the token <font color='blue'>Brooklyn</font> as a <font color='blue'>location</font>. We can also ask the <font color='blue'>pipeline</font> to <font color='blue'>group</font> together the <font color='blue'>tokens</font> that correspond to the <font color='blue'>same entity</font>:\n"
      ],
      "metadata": {
        "id": "5waqBVuLnMVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z_J430QnOFF",
        "outputId": "6e7b7b2c-7afa-4e1d-cf0c-e7bd541a1f0c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.9981694),\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': np.float32(0.9796019),\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': np.float32(0.9932106),\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <font color='blue'>`aggregation_strategy`</font> picked will <font color='blue'>change the scores</font> computed for each grouped entity. With <font color='blue'>simple</font> the <font color='blue'>score</font> is just the <font color='blue'>mean of the scores of each token</font> in the given entity: for instance, the score of <font color='blue'>Sylvain</font> is the <font color='blue'>mean of the scores</font> we saw in the <font color='blue'>previous example</font> for the tokens `S`, `##yl`, `##va`, and `##in`. Other strategies available are:\n",
        "\n",
        "- <font color='blue'>first</font>, where the <font color='blue'>score</font> of each entity is the <font color='blue'>score of the first token</font> of that entity (so for \"Sylvain\" it would be 0.993828, the score of the token `S`)\n",
        "- <font color='blue'>max</font>, where the score of each entity is the <font color='blue'>maximum score of the tokens</font> in that entity (so for \"Hugging Face\" it would be 0.98879766, the score of \"Face\")\n",
        "- <font color='blue'>average</font>, where the score of each entity is the <font color='blue'>average of the scores of the words</font> composing that entity (so for \"Sylvain\" there would be no difference from the `\"simple\"` strategy, but \"Hugging Face\" would have a score of 0.9819, the average of the scores for \"Hugging\", 0.975, and \"Face\", 0.98879)\n",
        "\n",
        "Now let's see how to obtain these results without using the `pipeline()` function!\n"
      ],
      "metadata": {
        "id": "KdsiagmwnQCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From inputs to predictions"
      ],
      "metadata": {
        "id": "DdN1nxY0nWy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to <font color='blue'>tokenize</font> our <font color='blue'>input</font> and <font color='blue'>pass</font> it <font color='blue'>through the model</font>. This is done exactly as in [Chapter 2](https://huggingface.co/learn/llm-course/chapter2/4); we instantiate the tokenizer and the model using the `AutoXxx` classes and then use them on our example:\n"
      ],
      "metadata": {
        "id": "lneuznHxnXuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "inputs = tokenizer(example, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwX_eFrhnTg0",
        "outputId": "9a77594f-e551-4baa-8cfa-a3f97278eed7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're using `AutoModelForTokenClassification` here, we get <font color='blue'>one set of logits</font> for <font color='blue'>each token</font> in the input sequence:"
      ],
      "metadata": {
        "id": "uHdVRL_wndEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[\"input_ids\"].shape)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1eUd-egndXe",
        "outputId": "61db3348-36c5-4eda-acb5-81d78524629a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 19])\n",
            "torch.Size([1, 19, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to <font color='blue'>tokenize our input</font> and pass it <font color='blue'>through the model</font>. This is done exactly as in [Chapter 2](/https://huggingface.co/learn/llm-course/chapter2/4); we <font color='blue'>instantiate</font> the <font color='blue'>tokenizer</font> and the <font color='blue'>model</font> using the <font color='blue'>`TFAutoXxx` classes</font> and then use them on our example:\n"
      ],
      "metadata": {
        "id": "0gAAT4rdnfM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "inputs = tokenizer(example, return_tensors=\"tf\")\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48o5eGUPnhJl",
        "outputId": "ea1baa63-55c6-471d-b8f3-d5aa891bf898"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're using `TFAutoModelForTokenClassification` here, we get <font color='blue'>one set of logits</font> for <font color='blue'>each token</font> in the input sequence:"
      ],
      "metadata": {
        "id": "Y2H4WmounjUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[\"input_ids\"].shape)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_0Tk7lJnlZY",
        "outputId": "e7a819be-1b96-4e56-fd1f-24fbe755285c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 19)\n",
            "(1, 19, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a batch with <font color='blue'>1 sequence of 19 tokens</font> and the model has <font color='blue'>9 different labels</font>, so the output of the model has a shape of <font color='blue'>`1 x 19 x 9`</font>. Like for the text classification pipeline, we use a <font color='blue'>softmax function</font> to <font color='blue'>convert</font> those <font color='blue'>logits to probabilities</font>, and we take the <font color='blue'>argmax</font> to get <font color='blue'>predictions</font> (note that we can take the argmax on the logits because the softmax does not change the order):\n"
      ],
      "metadata": {
        "id": "2M-O0rl0nok_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert TensorFlow tensor to PyTorch tensor before using torch.nn.functional.softmax\n",
        "logits_torch = torch.tensor(outputs.logits.numpy())\n",
        "probabilities = torch.nn.functional.softmax(logits_torch, dim=-1)[0].tolist()\n",
        "predictions = logits_torch.argmax(dim=-1)[0].tolist()\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV6sbozqnqwj",
        "outputId": "0350d883-e60a-42ba-9056-585d71da7b6b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]\n",
        "probabilities = probabilities.numpy().tolist()\n",
        "predictions = tf.math.argmax(outputs.logits, axis=-1)[0]\n",
        "predictions = predictions.numpy().tolist()\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LAi5pUloDxB",
        "outputId": "71951d42-ef07-410a-bda6-ade3b3410cc6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <font color='blue'>`model.config.id2label`</font> attribute contains the mapping of <font color='blue'>indexes to labels</font> that we can use to make sense of the predictions:"
      ],
      "metadata": {
        "id": "o_IVDbsCoGjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKSV3AxMoG4d",
        "outputId": "0715ba73-29d8-4a2b-aea7-721c5ca16e42"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-MISC',\n",
              " 2: 'I-MISC',\n",
              " 3: 'B-PER',\n",
              " 4: 'I-PER',\n",
              " 5: 'B-ORG',\n",
              " 6: 'I-ORG',\n",
              " 7: 'B-LOC',\n",
              " 8: 'I-LOC'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, there are <font color='blue'>9 labels</font>: <font color='blue'>`O`</font> is the label for the <font color='blue'>tokens</font> that are <font color='blue'>not in any named entity</font> (it stands for \"outside\"), and we then have <font color='blue'>two labels</font> for <font color='blue'>each type of entity</font> (miscellaneous, person, organization, and location). The label <font color='blue'>`B-XXX`</font> indicates the <font color='blue'>token</font> is at the <font color='blue'>beginning</font> of an entity `XXX` and the label <font color='blue'>`I-XXX`</font> indicates the <font color='blue'>token</font> is <font color='blue'>inside</font> the entity `XXX`. For instance, in the current example we would expect our model to classify the token <font color='blue'>`S`</font> as <font color='blue'>`B-PER`</font> (beginning of a person entity) and the tokens <font color='blue'>`##yl`, `##va`</font> and <font color='blue'>`##in`</font> as <font color='blue'>`I-PER`</font> (inside a person entity).\n",
        "\n",
        "You might think the model was wrong in this case as it gave the label <font color='blue'>`I-PER`</font> to <font color='blue'>all four</font> of these <font color='blue'>tokens</font>, but that's not entirely true. There are actually two formats for those `B-` and `I-` labels: <font color='blue'>IOB1</font> and <font color='blue'>IOB2</font>. The <font color='blue'>IOB2</font> format (in <font color='blue'>pink</font> below), is the one we introduced whereas in the <font color='blue'>IOB1</font> format (in <font color='blue'>blue</font>), the labels <font color='blue'>beginning</font> with <font color='blue'>`B-`</font> are only ever used to <font color='blue'>separate two adjacent entities</font> of the <font color='blue'>same type</font>. The model we are using was <font color='blue'>fine-tuned</font> on a <font color='blue'>dataset</font> using <font color='blue'>that format</font>, which is why it assigns the label `I-PER` to the `S` token.\n",
        "\n",
        "<div class=\"flex justify-center\">\n",
        "<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg\" alt=\"IOB1 vs IOB2 format\"/>\n",
        "</div>\n",
        "\n",
        "With this map, we are ready to <font color='blue'>reproduce</font> (almost entirely) the <font color='blue'>results of the first pipeline</font> -- we can just grab the score and label of each token that was not classified as `O`:"
      ],
      "metadata": {
        "id": "VrdD6_VdoJAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "tokens = inputs.tokens()\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        results.append(\n",
        "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
        "        )\n",
        "\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8kp0NVpoNwv",
        "outputId": "b9905fb0-4469-4e8b-8c91-3c0959526d40"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entity': 'I-PER', 'score': 0.9993829131126404, 'word': 'S'}\n",
            "{'entity': 'I-PER', 'score': 0.998154878616333, 'word': '##yl'}\n",
            "{'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}\n",
            "{'entity': 'I-PER', 'score': 0.9992326498031616, 'word': '##in'}\n",
            "{'entity': 'I-ORG', 'score': 0.9738930463790894, 'word': 'Hu'}\n",
            "{'entity': 'I-ORG', 'score': 0.9761150479316711, 'word': '##gging'}\n",
            "{'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'}\n",
            "{'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is very similar to what we had before, with one exception: the <font color='blue'>pipeline</font> also gave us <font color='blue'>information</font> about the <font color='blue'>`start` and `end`</font> of each entity in the original sentence. This is where our <font color='blue'>offset mapping</font> will come into play. To get the offsets, we just have to set <font color='blue'>`return_offsets_mapping=True`</font> when we apply the tokenizer to our inputs:\n"
      ],
      "metadata": {
        "id": "zUD7FLYxoZNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "inputs_with_offsets[\"offset_mapping\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrKy4R4goZtt",
        "outputId": "90ad84a2-6578-4746-e5b1-6fc8b83f9e06"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (0, 2),\n",
              " (3, 7),\n",
              " (8, 10),\n",
              " (11, 12),\n",
              " (12, 14),\n",
              " (14, 16),\n",
              " (16, 18),\n",
              " (19, 22),\n",
              " (23, 24),\n",
              " (25, 29),\n",
              " (30, 32),\n",
              " (33, 35),\n",
              " (35, 40),\n",
              " (41, 45),\n",
              " (46, 48),\n",
              " (49, 57),\n",
              " (57, 58),\n",
              " (0, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each <font color='blue'>tuple</font> is the <font color='blue'>span of text</font> corresponding to <font color='blue'>each token</font>, where `(0, 0)` is reserved for the special tokens. We saw before that the token at index 5 is `##yl`, which has <font color='blue'>`(12, 14)`</font> as <font color='blue'>offsets</font> here. If we grab the corresponding slice in our example:\n"
      ],
      "metadata": {
        "id": "l8X-m4VdodK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example[12:14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B6i1B_zOodd0",
        "outputId": "eeb3e621-1770-4e76-b2a2-b372557e4a37"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we get the <font color='blue'>proper span</font> of <font color='blue'>text</font> without the `##`. Using this, we can now complete the previous results:"
      ],
      "metadata": {
        "id": "xUYf67FuogIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        start, end = offsets[idx]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity\": label,\n",
        "                \"score\": probabilities[idx][pred],\n",
        "                \"word\": tokens[idx],\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-HJcnjVojpG",
        "outputId": "bc22d622-a1f1-4e14-c59c-35a5295c11c6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entity': 'I-PER', 'score': 0.9993829131126404, 'word': 'S', 'start': 11, 'end': 12}\n",
            "{'entity': 'I-PER', 'score': 0.998154878616333, 'word': '##yl', 'start': 12, 'end': 14}\n",
            "{'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}\n",
            "{'entity': 'I-PER', 'score': 0.9992326498031616, 'word': '##in', 'start': 16, 'end': 18}\n",
            "{'entity': 'I-ORG', 'score': 0.9738930463790894, 'word': 'Hu', 'start': 33, 'end': 35}\n",
            "{'entity': 'I-ORG', 'score': 0.9761150479316711, 'word': '##gging', 'start': 35, 'end': 40}\n",
            "{'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face', 'start': 41, 'end': 45}\n",
            "{'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same as what we got from the first pipeline!"
      ],
      "metadata": {
        "id": "wf6kTOxconLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping entities"
      ],
      "metadata": {
        "id": "hXVjz18CooW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the <font color='blue'>offsets</font> to determine the <font color='blue'>start and end keys</font> for <font color='blue'>each entity</font> is <font color='blue'>handy</font>, but that information isn't strictly necessary. When we want to <font color='blue'>group the entities together</font>, however, the <font color='blue'>offsets</font> will save us a lot of <font color='blue'>messy code</font>. For example, if we wanted to group together the tokens `Hu`, `##gging`, and `Face`, we could make special rules that say the first two should be attached while removing the `##`, and the `Face` should be added with a space since it does not begin with `##` -- but that would only work for this particular type of tokenizer. We would have to write <font color='blue'>another set of rules</font> for a <font color='blue'>SentencePiece</font> or a <font color='blue'>Byte-Pair-Encoding tokenizer</font> (discussed later in this chapter).\n",
        "\n",
        "With the <font color='blue'>offsets</font>, all that <font color='blue'>custom code goes away</font>: we just can take the <font color='blue'>span in the original text</font> that <font color='blue'>begins</font> with the <font color='blue'>first token</font> and <font color='blue'>ends</font> with the <font color='blue'>last token</font>. So, in the case of the tokens `Hu`, `##gging`, and `Face`, we should start at character 33 (the beginning of `Hu`) and end before character 45 (the end of `Face`):\n"
      ],
      "metadata": {
        "id": "OVRIEKm3oqir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example[33:45]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ynp4XrSPonqz",
        "outputId": "15187af5-ae8e-40f8-e591-35a2383457a5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hugging Face'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To write the code that <font color='blue'>post-processes</font> the <font color='blue'>predictions</font> while <font color='blue'>grouping entities</font>, we will <font color='blue'>group together entities</font> that are <font color='blue'>consecutive</font> and <font color='blue'>labeled with `I-XXX`</font>, <font color='blue'>except</font> for the <font color='blue'>first one</font>, which can be <font color='blue'>labeled</font> as <font color='blue'>`B-XXX` or `I-XXX`</font> (so, we stop grouping an entity when we get a `O`, a new type of entity, or a `B-XXX` that tells us an entity of the same type is starting):\n"
      ],
      "metadata": {
        "id": "LO_tIVzoouh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "idx = 0\n",
        "while idx < len(predictions):\n",
        "    pred = predictions[idx]\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        # Remove the B- or I-\n",
        "        label = label[2:]\n",
        "        start, _ = offsets[idx]\n",
        "\n",
        "        # Grab all the tokens labeled with I-label\n",
        "        all_scores = []\n",
        "        while (\n",
        "            idx < len(predictions)\n",
        "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
        "        ):\n",
        "            all_scores.append(probabilities[idx][pred])\n",
        "            _, end = offsets[idx]\n",
        "            idx += 1\n",
        "\n",
        "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
        "        score = np.mean(all_scores).item()\n",
        "        word = example[start:end]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity_group\": label,\n",
        "                \"score\": score,\n",
        "                \"word\": word,\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "    idx += 1\n",
        "\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b0Qgx6howsC",
        "outputId": "88e164dd-980f-4dfc-8787-c4d6bddcd6dd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entity_group': 'PER', 'score': 0.998169407248497, 'word': 'Sylvain', 'start': 11, 'end': 18}\n",
            "{'entity_group': 'ORG', 'score': 0.9796018600463867, 'word': 'Hugging Face', 'start': 33, 'end': 45}\n",
            "{'entity_group': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we get the <font color='blue'>same results</font> as with our <font color='blue'>second pipeline</font>! <font color='blue'>Another example</font> of a task where these offsets are extremely useful is <font color='blue'>question answering</font>. Diving into that pipeline, which we'll do in the next section, will also enable us to take a look at one last feature of the tokenizers in the 🤗 Transformers library: <font color='blue'>dealing</font> with <font color='blue'>overflowing tokens</font> when we <font color='blue'>truncate an input</font> to a given length.\n"
      ],
      "metadata": {
        "id": "dd7gwLg-o0Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-6-66qfEWXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}