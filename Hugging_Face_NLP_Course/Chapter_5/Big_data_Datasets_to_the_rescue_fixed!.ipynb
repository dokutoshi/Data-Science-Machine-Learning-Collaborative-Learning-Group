{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmocevJpb6rC"
      },
      "source": [
        "# Big data? ü§ó Datasets to the rescue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AZge_xSb6rD"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh8vURJEb6rH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, log into Hugging face."
      ],
      "metadata": {
        "id": "hHnzJTaHeJEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "MiJBGV-FeJ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nowadays it is not uncommon to find yourself working with <font color='blue'>multi-gigabyte datasets</font>, especially if you're planning to <font color='blue'>pretrain a transformer</font> like <font color='blue'>BERT</font> or <font color='blue'>GPT-2</font> from scratch. In these cases, even <font color='blue'>loading the data</font> can be a <font color='blue'>challenge</font>. For example, the <font color='blue'>WebText corpus</font> used to pretrain GPT-2 consists of over <font color='blue'>8 million documents</font> and <font color='blue'>40 GB of text</font> -- loading this into your laptop's RAM is likely to give it a heart attack!\n",
        "\n",
        "Fortunately, ü§ó Datasets has been designed to overcome these limitations. It frees you from <font color='blue'>memory management</font> problems by treating <font color='blue'>datasets as _memory-mapped_ files</font>, and from <font color='blue'>hard drive limits</font> by _streaming_ the entries in a corpus.\n",
        "\n",
        "<Youtube id=\"JwISwTCPPWo\"/>\n",
        "\n",
        "In this section we'll explore these features of ü§ó Datasets with a huge 825 GB corpus known as [the Pile](https://pile.eleuther.ai). Let's get started!"
      ],
      "metadata": {
        "id": "Nsly8R_YcC6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the Pile?\n",
        "\n",
        "The Pile is an <font color='blue'>English text corpus</font> that was created by [EleutherAI](https://www.eleuther.ai) for training large-scale language models. It includes a <font color='blue'>diverse range of datasets</font>, spanning <font color='blue'>scientific articles</font>, GitHub <font color='blue'>code repositories</font>, and filtered <font color='blue'>web text</font>. The training corpus is available in [14 GB chunks](https://the-eye.eu/public/AI/pile/), and you can also download several of the [individual components](https://the-eye.eu/public/AI/pile_preliminary_components/). Let's start by taking a look at the <font color='blue'>PubMed Abstracts dataset</font>, which is a corpus of <font color='blue'>abstracts</font> from <font color='blue'>15 million biomedical publications</font> on [PubMed](https://pubmed.ncbi.nlm.nih.gov/). The dataset is in [JSON Lines format](https://jsonlines.org) and is compressed using the `zstandard` library, so first we need to install that:"
      ],
      "metadata": {
        "id": "_xXn3tAmcGr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1sLn8-7b6rI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install zstandard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can <font color='blue'>load the dataset</font> using the method for remote files that we learned in [section 2](https://huggingface.co/course/chapter5/2):"
      ],
      "metadata": {
        "id": "7rHRZN2-cOdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNV-lhiPb6rI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DownloadConfig\n",
        "\n",
        "data_files = \"https://huggingface.co/datasets/casinca/PUBMED_title_abstracts_2019_baseline/resolve/main/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
        "pubmed_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=data_files,\n",
        "    split=\"train\",\n",
        "    download_config=DownloadConfig(delete_extracted=True),  # optional argument\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset.shape"
      ],
      "metadata": {
        "id": "7sGLXc_LiKb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are <font color='blue'>15,518,009 rows and 2 columns</font> in our dataset -- that's a lot!\n",
        "\n",
        "<Tip>\n",
        "\n",
        "‚úé By default, ü§ó Datasets will decompress the files needed to load a dataset. If you want to preserve hard drive space, you can pass `DownloadConfig(delete_extracted=True)` to the `download_config` argument of `load_dataset()`. See the [documentation](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig) for more details.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Let's inspect the contents of the first example:\n"
      ],
      "metadata": {
        "id": "ruwPUkErcThE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi6J7Dajb6rJ"
      },
      "outputs": [],
      "source": [
        "pubmed_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, this looks like the <font color='blue'>abstract</font> from a <font color='blue'>medical article</font>. Now let's see how much RAM we've used to load the dataset!\n",
        "\n",
        "## The magic of memory mapping\n",
        "\n",
        "A simple way to <font color='blue'>measure memory usage</font> in Python is with the [`psutil`](https://psutil.readthedocs.io/en/latest/) library, which can be installed with `pip` as follows:"
      ],
      "metadata": {
        "id": "N_RiU6JpcfiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YM1ETUpb6rK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It provides a `Process` class that allows us to check the memory usage of the current process as follows"
      ],
      "metadata": {
        "id": "l9FKKUHfcmlU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F4DaVwmb6rK"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
        "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the `rss` attribute refers to the <font color='blue'>resident set size</font>, which is the <font color='blue'>fraction of memory</font> that a process occupies in <font color='blue'>RAM</font>. This measurement also includes the memory used by the Python interpreter and the libraries we've loaded, so the actual amount of memory used to load the dataset is a bit smaller. For comparison, let's see how <font color='blue'>large the dataset</font> is on <font color='blue'>disk</font>, using the `dataset_size` attribute. Since the result is expressed in bytes like before, we need to manually convert it to gigabytes:"
      ],
      "metadata": {
        "id": "F3ZqSAHrcqra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz4aggCmb6rL"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\n",
        "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
        "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice -- despite it being almost 20 GB large, we're able to load and access the dataset with much less RAM!\n",
        "\n",
        "<Tip>\n",
        "\n",
        "‚úèÔ∏è **Try it out!** Pick one of the [subsets](https://the-eye.eu/public/AI/pile_preliminary_components/) from the <font color='blue'>Pile</font> that is <font color='blue'>larger</font> than your <font color='blue'>laptop or desktop's RAM</font>, load it with ü§ó Datasets, and measure the amount of RAM used. Note that to get an accurate measurement, you'll want to do this in a new process. You can find the decompressed sizes of each subset in Table 1 of [the Pile paper](https://arxiv.org/abs/2101.00027).\n",
        "\n",
        "</Tip>\n"
      ],
      "metadata": {
        "id": "WjyDV92Ocs_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the C4 dataset\n",
        "dataset_name = \"allenai/c4\"\n",
        "config_name = \"en\"\n",
        "\n",
        "# Load the dataset\n",
        "c4_dataset = load_dataset(\n",
        "    dataset_name,\n",
        "    config_name,\n",
        "    split=\"train\",\n",
        "    streaming=True  # Streaming to handle large dataset efficiently\n",
        ")"
      ],
      "metadata": {
        "id": "6a-a96zlm7EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure RAM usage after initializing the dataset\n",
        "print(f\"RAM used before accessing elements: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
      ],
      "metadata": {
        "id": "WEOYbe-TnE14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a few elements to trigger the data loading\n",
        "sample_elements = [next(iter(c4_dataset)) for _ in range(5)]\n",
        "\n",
        "# Print each element\n",
        "for i, element in enumerate(sample_elements):\n",
        "    print(f\"Sample element {i + 1}:{element}\\n\")"
      ],
      "metadata": {
        "id": "Zunhw1H-nIkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate the dataset size if feasible\n",
        "if hasattr(c4_dataset, 'dataset_size'):\n",
        "    size_gb = c4_dataset.dataset_size / (1024**3)\n",
        "    print(f\"Dataset size (cache file): {size_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"Dataset size attribute not available.\")"
      ],
      "metadata": {
        "id": "cFj1bmpnpH8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're familiar with Pandas, this result might come as a surprise because of Wes Kinney's famous [rule of thumb](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) that you typically need <font color='blue'>5 to 10 times</font> as much <font color='blue'>RAM</font> as the <font color='blue'>size of your dataset</font>. So how does ü§ó Datasets solve this memory management problem? ü§ó Datasets treats each dataset as a [memory-mapped file](https://en.wikipedia.org/wiki/Memory-mapped_file), which provides a <font color='blue'>mapping</font> between <font color='blue'>RAM</font> and <font color='blue'>filesystem storage</font> that allows the library to access and operate on elements of the dataset without needing to fully load it into memory.\n",
        "\n",
        "<font color='blue'>Memory-mapped files</font> can also be <font color='blue'>shared across multiple processes</font>, which enables methods like `Dataset.map()` to be parallelized without needing to move or copy the dataset. Under the hood, these capabilities are all realized by the [Apache Arrow](https://arrow.apache.org) memory format and [`pyarrow`](https://arrow.apache.org/docs/python/index.html) library, which make the data loading and processing lightning fast. (For more details about Apache Arrow and comparisons to Pandas, check out [Dejan Simic's blog post](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) To see this in action, let's run a little speed test by iterating over all the elements in the PubMed Abstracts dataset:"
      ],
      "metadata": {
        "id": "PodEfxWYm7Kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYcOU4Lab6rL"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "code_snippet = \"\"\"batch_size = 1000\n",
        "\n",
        "for idx in range(0, len(pubmed_dataset), batch_size):\n",
        "    _ = pubmed_dataset[idx:idx + batch_size]\n",
        "\"\"\"\n",
        "\n",
        "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
        "print(\n",
        "    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
        "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we've used Python's `timeit` module to measure the execution time taken by `code_snippet`. You'll typically be able to iterate over a dataset at speed of a <font color='blue'>few tenths of a GB/s to several GB/s</font>. This works great for the vast majority of applications, but sometimes you'll have to work with a dataset that is too large to even store on your laptop's hard drive. For example, if we tried to download the Pile in its entirety, we'd need 825 GB of free disk space! To handle these cases, ü§ó Datasets provides a <font color='blue'>streaming feature</font> that allows us to <font color='blue'>download and access elements on the fly</font>, without needing to download the whole dataset. Let's take a look at how this works.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "üí° In Jupyter notebooks you can also time cells using the [`%%timeit` magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "IlPmmKDmc14P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming datasets\n",
        "\n",
        "To <font color='blue'>enable dataset streaming</font> you just need to pass the `streaming=True` argument to the `load_dataset()` function. For example, let's load the PubMed Abstracts dataset again, but in streaming mode:"
      ],
      "metadata": {
        "id": "TYHBghoXc37G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69jX8erbb6rM"
      },
      "outputs": [],
      "source": [
        "pubmed_dataset_streamed = load_dataset(\n",
        "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of the familiar `Dataset` that we've encountered elsewhere in this chapter, the <font color='blue'>object returned</font> with `streaming=True` is an <font color='blue'>IterableDataset</font>. As the name suggests, to access the elements of an `IterableDataset` we need to iterate over it. We can access the first element of our streamed dataset as follows:\n"
      ],
      "metadata": {
        "id": "5k7LxDptc6Rb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o37UHuvVb6rM"
      },
      "outputs": [],
      "source": [
        "next(iter(pubmed_dataset_streamed))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elements from a streamed dataset can be <font color='blue'>processed on the fly</font> using <font color='blue'>`IterableDataset.map()`</font>, which is <font color='blue'>useful during training</font> if you need to <font color='blue'>tokenize the inputs</font>. The process is exactly the same as the one we used to tokenize our dataset in [Chapter 3](https://huggingface.co/course/chapter3), with the only difference being that outputs are returned one by one:"
      ],
      "metadata": {
        "id": "pM25AzJ_c8BS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw15ExZNb6rN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
        "item = next(iter(tokenized_dataset))\n",
        "\n",
        "def format_item(item):\n",
        "    print(\"META:\")\n",
        "    print(item['meta'])\n",
        "\n",
        "    print(\"\\nTEXT:\")\n",
        "    paragraphs = item['text'].split('\\n')\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "        print(f\"Paragraph {i+1}:\")\n",
        "        import textwrap\n",
        "        wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "        print(wrapped_text)\n",
        "        print()\n",
        "\n",
        "    # Format input_ids and attention mask\n",
        "    print(\"\\nINPUT_IDS (first 520 tokens):\")\n",
        "    tokens = tokenizer.convert_ids_to_tokens(item['input_ids'][:20])\n",
        "    for i, (token_id, token) in enumerate(zip(item['input_ids'][:20], tokens)):\n",
        "        print(f\"{i:3d}: {token_id:6d} -> '{token}'\")\n",
        "\n",
        "    if 'attention_mask' in item:\n",
        "        print(\"\\nATTENTION_MASK (first 20 values):\")\n",
        "        for i, mask in enumerate(item['attention_mask'][:20]):\n",
        "            print(f\"{i:3d}: {mask}\")\n",
        "\n",
        "    print(f\"\\nTotal length: {len(item['input_ids'])} tokens\")\n",
        "\n",
        "format_item(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<Tip>\n",
        "\n",
        "üí° To <font color='blue'>speed up tokenization</font> with <font color='blue'>streaming</font> you can pass <font color='blue'>`batched=True`</font>, as we saw in the last section. It will process the examples batch by batch; the <font color='blue'>default batch size is 1,000</font> and can be specified with the `batch_size` argument.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You can also <font color='blue'>shuffle a streamed dataset</font> using `IterableDataset.shuffle()`, but unlike `Dataset.shuffle()` this only shuffles the elements in a predefined `buffer_size`:\n"
      ],
      "metadata": {
        "id": "vNH3CRRsdA8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH3W9VP5b6rN"
      },
      "outputs": [],
      "source": [
        "# shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
        "# next(iter(shuffled_dataset))\n",
        "\n",
        "def display_shuffled_item(item):\n",
        "    \"\"\"\n",
        "    Displays a single item from a shuffled dataset in a readable format.\n",
        "\n",
        "    Args:\n",
        "        item: Dataset item to display\n",
        "    \"\"\"\n",
        "    import textwrap\n",
        "\n",
        "    print(\"META:\")\n",
        "    for key, value in item['meta'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\nTEXT:\")\n",
        "    paragraphs = item['text'].split('\\n')\n",
        "    for j, paragraph in enumerate(paragraphs):\n",
        "        print(f\"\\nParagraph {j+1}:\")\n",
        "        wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "        print(wrapped_text)\n",
        "\n",
        "    if 'input_ids' in item:\n",
        "        print(\"\\nTOKEN INFORMATION:\")\n",
        "        print(f\"Total tokens: {len(item['input_ids'])}\")\n",
        "\n",
        "        print(f\"\\nFirst 10 tokens:\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(item['input_ids'][:10])\n",
        "        for j, (token_id, token) in enumerate(zip(item['input_ids'][:10], tokens)):\n",
        "            print(f\"  {j:3d}: {token_id:6d} -> '{token}'\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
        "sample_item = next(iter(shuffled_dataset))\n",
        "display_shuffled_item(sample_item)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we selected a <font color='blue'>random example from the first 10,000 examples</font> in the buffer. <font color='blue'>Once</font> an <font color='blue'>example</font> is <font color='blue'>accessed</font>, its <font color='blue'>spot in the buffer</font> is <font color='blue'>filled</font> with the <font color='blue'>next example</font> in the corpus (i.e., the 10,001st example in the case above). You can also select elements from a streamed dataset using the `IterableDataset.take()` and `IterableDataset.skip()` functions, which act in a similar way to `Dataset.select()`. For example, to <font color='blue'>select the first 5 examples</font> in the PubMed Abstracts dataset we can do the following:"
      ],
      "metadata": {
        "id": "HnX3RsrKdEwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBeS0P0Xb6rN"
      },
      "outputs": [],
      "source": [
        "def display_dataset(dataset_list):\n",
        "    \"\"\"\n",
        "    Displays a list of dataset items in a readable format.\n",
        "\n",
        "    Args:\n",
        "        dataset_list: List of dataset items\n",
        "    \"\"\"\n",
        "    import textwrap\n",
        "\n",
        "\n",
        "    for i, item in enumerate(dataset_list):\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"ITEM {i+1}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        print(\"META:\")\n",
        "        for key, value in item['meta'].items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "        print(\"\\nTEXT:\")\n",
        "        paragraphs = item['text'].split('\\n')\n",
        "        for j, paragraph in enumerate(paragraphs):\n",
        "            print(f\"\\nParagraph {j+1}:\")\n",
        "            wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "            print(wrapped_text)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "dataset_head = pubmed_dataset_streamed.take(5)\n",
        "dataset_list = list(dataset_head)\n",
        "display_dataset(dataset_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, you can use the `IterableDataset.skip()` function to <font color='blue'>create training and validation splits</font> from a shuffled dataset as follows:"
      ],
      "metadata": {
        "id": "IKCrKuV9dHHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkaE-SILb6rO"
      },
      "outputs": [],
      "source": [
        "# Skip the first 1,000 examples and include the rest in the training set\n",
        "train_dataset = shuffled_dataset.skip(1000)\n",
        "# Take the first 1,000 examples for the validation set\n",
        "validation_dataset = shuffled_dataset.take(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's round out our exploration of dataset <font color='blue'>streaming</font> with a <font color='blue'>common application</font>: <font color='blue'>combining multiple datasets together</font> to create a <font color='blue'>single corpus</font>. ü§ó Datasets provides an `interleave_datasets()` function that converts a list of `IterableDataset` objects into a single `IterableDataset`, where the elements of the new dataset are obtained by <font color='blue'>alternating among the source examples</font>. This function is especially useful when you're trying to combine large datasets, so as an example let's stream the FreeLaw subset of the Pile, which is a 51 GB dataset of legal opinions from US courts:"
      ],
      "metadata": {
        "id": "Idh5Ns7NdJJF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv0Uyi7mb6rO"
      },
      "outputs": [],
      "source": [
        "# The link to the Law dataset is invalid!\n",
        "from datasets import load_dataset, DownloadConfig\n",
        "\n",
        "# Load the BookCorpus dataset from Hugging Face\n",
        "dataset_name = \"bookcorpus\"\n",
        "\n",
        "bookcorpus_dataset_streamed = load_dataset(\n",
        "    dataset_name,\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate the dataset size if feasible\n",
        "if hasattr(bookcorpus_dataset_streamed, 'dataset_size'):\n",
        "    size_gb = bookcorpus_dataset_streamed.dataset_size / (1024**3)\n",
        "    print(f\"Dataset size (cache file): {size_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"Dataset size attribute not available.\")"
      ],
      "metadata": {
        "id": "YdIfIubnrSAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is large enough to stress the RAM of most laptops, yet we've been able to load and access it without breaking a sweat! Let's now combine the examples from the <font color='blue'>FreeLaw</font> and <font color='blue'>PubMed Abstracts datasets</font> with the `interleave_datasets()` function:"
      ],
      "metadata": {
        "id": "18pl9OoLdL3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "from datasets import interleave_datasets\n",
        "import json\n",
        "\n",
        "combined_dataset = interleave_datasets([pubmed_dataset_streamed, bookcorpus_dataset_streamed])\n",
        "sampled_data = list(islice(combined_dataset, 2))\n",
        "\n",
        "for i, entry in enumerate(sampled_data):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(json.dumps(entry, indent=4, ensure_ascii=False))\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "9jYqkkjnqQJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we've used the `islice()` function from Python's `itertools` module to select the <font color='blue'>first two examples</font> from the <font color='blue'>combined dataset</font>, and we can see that they match the first examples from each of the two source datasets.\n",
        "\n",
        "Finally, if you want to stream the <font color='blue'>Pile in its 825 GB entirety</font>, you can grab all the prepared files as follows:"
      ],
      "metadata": {
        "id": "tbrgbayRdP7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI89R0wxb6rO"
      },
      "outputs": [],
      "source": [
        "# Loading the Pile in its entirety definitely didn't work\n",
        "\n",
        "# Load the English Wikipedia dataset\n",
        "wikipedia_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a few elements\n",
        "sample_elements = [next(iter(wikipedia_dataset)) for _ in range(5)]\n",
        "for i, element in enumerate(sample_elements):\n",
        "    print(f\"Sample element {i + 1}:{element}\\n\")"
      ],
      "metadata": {
        "id": "sAcZ7tD4ugWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate the dataset size if feasible\n",
        "if hasattr(wikipedia_dataset, 'dataset_size'):\n",
        "    size_gb = wikipedia_dataset.dataset_size / (1024**3)\n",
        "    print(f\"Dataset size (cache file): {size_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"Dataset size attribute not available.\")"
      ],
      "metadata": {
        "id": "2oPoZrBmumiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<Tip>\n",
        "\n",
        "‚úèÔ∏è **Try it out!** Use one of the large Common Crawl corpora like [`mc4`](https://huggingface.co/datasets/mc4) or [`oscar`](https://huggingface.co/datasets/oscar) to create a <font color='blue'>streaming multilingual dataset</font> that represents the spoken proportions of languages in a country of your choice. For example, the four national languages in Switzerland are German, French, Italian, and Romansh, so you could try creating a Swiss corpus by sampling the Oscar subsets according to their spoken proportion.\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "73SldmIudSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "\n",
        "# Define language proportions for Switzerland\n",
        "language_proportions = {\n",
        "    \"de\": 0.635,  # German\n",
        "    \"fr\": 0.225,  # French\n",
        "    \"it\": 0.081,  # Italian\n",
        "    \"rm\": 0.005   # Romansh\n",
        "}\n",
        "\n",
        "# Load OSCAR dataset subsets for the specified languages\n",
        "languages = list(language_proportions.keys())\n",
        "oscar_datasets = {lang: load_dataset(\"oscar\", f\"unshuffled_deduplicated_{lang}\", split=\"train\", streaming=True) for lang in languages}"
      ],
      "metadata": {
        "id": "blfjZGGcu3dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_elements(datasets, proportions, num_samples=100):\n",
        "    \"\"\"\n",
        "    Sample elements from the given datasets according to the given proportions.\n",
        "    \"\"\"\n",
        "    sampled_elements = []\n",
        "    total_samples = 0\n",
        "\n",
        "    while total_samples < num_samples:\n",
        "        for lang, prop in proportions.items():\n",
        "            num_lang_samples = int(num_samples * prop)\n",
        "            lang_dataset = datasets[lang]\n",
        "\n",
        "            try:\n",
        "                for _ in range(num_lang_samples):\n",
        "                    sample = next(iter(lang_dataset))\n",
        "                    if sample not in sampled_elements:\n",
        "                        sampled_elements.append((lang, sample))\n",
        "            except StopIteration:\n",
        "                continue\n",
        "\n",
        "            total_samples += num_lang_samples\n",
        "\n",
        "            if total_samples >= num_samples:\n",
        "                break\n",
        "\n",
        "    return sampled_elements\n",
        "\n",
        "sampled_elements = sample_elements(oscar_datasets, language_proportions, num_samples=1000)"
      ],
      "metadata": {
        "id": "PwAhg-LCu-fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some of the sampled elements\n",
        "for i, (lang, element) in enumerate(sampled_elements[:5]):\n",
        "    print(f\"Sample {i + 1} (Language: {lang}):{element}\\n\")"
      ],
      "metadata": {
        "id": "dWAw1WhgvFY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure RAM usage\n",
        "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "# Estimate the dataset size if feasible\n",
        "if hasattr(wikipedia_dataset, 'dataset_size'):\n",
        "    size_gb = wikipedia_dataset.dataset_size / (1024**3)\n",
        "    print(f\"Dataset size (cache file): {size_gb:.2f} GB\")\n",
        "else:\n",
        "    print(\"Dataset size attribute not available.\")"
      ],
      "metadata": {
        "id": "f6XivHupvRMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have all the tools you need to <font color='blue'>load and process datasets of all shapes and sizes</font> -- but unless you're exceptionally lucky, there will come a point in your NLP journey where you'll have to actually create a dataset to solve the problem at hand. That's the topic of the next section!"
      ],
      "metadata": {
        "id": "qMl5dMvIq_1q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}